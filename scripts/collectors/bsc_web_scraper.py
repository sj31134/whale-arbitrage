#!/usr/bin/env python3
"""
BSC Web Scraper Module

BSCscan ì›¹ì‚¬ì´íŠ¸ì—ì„œ ì¶”ê°€ ê±°ë˜ ì •ë³´ë¥¼ ìŠ¤í¬ë˜í•‘í•˜ëŠ” ëª¨ë“ˆ
- íŠ¹ì • tx_hashì˜ ìƒì„¸ í˜ì´ì§€ ìŠ¤í¬ë˜í•‘
- Method, Label, Direction ë“± ì¶”ê°€ ì •ë³´ ì¶”ì¶œ
- Rate limiting ê´€ë¦¬
"""

import time
import re
from typing import Dict, Optional, Tuple
from bs4 import BeautifulSoup
import requests

# ì„¤ì •
BSCSCAN_BASE_URL = "https://bscscan.com"
MAX_RETRIES = 3
RETRY_DELAY = 2  # ì´ˆ
REQUEST_TIMEOUT = 30  # ì´ˆ


def get_headers():
    """
    ìš”ì²­ í—¤ë” ì„¤ì • - ë´‡ìœ¼ë¡œ ê°ì§€ë˜ì§€ ì•Šë„ë¡ User-Agent ì„¤ì •
    """
    return {
        'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7',
        'Accept-Encoding': 'gzip, deflate, br',
        'Connection': 'keep-alive',
        'Upgrade-Insecure-Requests': '1',
        'Referer': 'https://bscscan.com/',
        'DNT': '1',
        'Sec-Fetch-Dest': 'document',
        'Sec-Fetch-Mode': 'navigate',
        'Sec-Fetch-Site': 'same-origin'
    }


def clean_text(text):
    """
    í…ìŠ¤íŠ¸ì—ì„œ ë¶ˆí•„ìš”í•œ ê³µë°±ê³¼ íŠ¹ìˆ˜ ë¬¸ì ì œê±°
    """
    if text is None:
        return ""
    return ' '.join(text.strip().split())


def extract_method(soup: BeautifulSoup) -> Optional[str]:
    """
    ê±°ë˜ Method ì¶”ì¶œ
    
    ì˜ˆ: Transfer, Swap, Approve ë“±
    """
    try:
        # Method ì •ë³´ëŠ” ì—¬ëŸ¬ ìœ„ì¹˜ì— ìˆì„ ìˆ˜ ìˆìŒ
        # 1. span.u-label íƒœê·¸
        method_span = soup.find('span', class_='u-label')
        if method_span:
            method = clean_text(method_span.get_text())
            if method:
                return method
        
        # 2. input dataì—ì„œ function signature ì¶”ì¶œ
        input_data_section = soup.find('div', id='inputdata')
        if input_data_section:
            # Function signature ì°¾ê¸°
            func_match = re.search(r'Function:\s*([^\n]+)', input_data_section.get_text())
            if func_match:
                return clean_text(func_match.group(1))
        
        return None
    
    except Exception as e:
        print(f"âš ï¸ Method ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None


def extract_labels(soup: BeautifulSoup) -> Tuple[Optional[str], Optional[str]]:
    """
    From/To ì£¼ì†Œì˜ ë¼ë²¨ ì¶”ì¶œ
    
    Returns:
    --------
    Tuple[Optional[str], Optional[str]] : (from_label, to_label)
    """
    try:
        from_label = None
        to_label = None
        
        # From/To ì„¹ì…˜ ì°¾ê¸°
        rows = soup.find_all('div', class_='row')
        
        for row in rows:
            # From ë¼ë²¨ ì°¾ê¸°
            if 'From:' in row.get_text():
                label_span = row.find('span', attrs={'data-bs-toggle': 'tooltip'})
                if label_span:
                    from_label = clean_text(label_span.get_text())
                    # ì£¼ì†Œê°€ ì•„ë‹Œ ë¼ë²¨ì¸ ê²½ìš°ë§Œ
                    if not from_label.startswith('0x'):
                        pass
                    else:
                        from_label = None
            
            # To ë¼ë²¨ ì°¾ê¸°
            if 'To:' in row.get_text():
                label_span = row.find('span', attrs={'data-bs-toggle': 'tooltip'})
                if label_span:
                    to_label = clean_text(label_span.get_text())
                    # ì£¼ì†Œê°€ ì•„ë‹Œ ë¼ë²¨ì¸ ê²½ìš°ë§Œ
                    if not to_label.startswith('0x'):
                        pass
                    else:
                        to_label = None
        
        return from_label, to_label
    
    except Exception as e:
        print(f"âš ï¸ Label ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None, None


def extract_direction(soup: BeautifulSoup, target_address: str) -> Optional[str]:
    """
    ê±°ë˜ ë°©í–¥ ì¶”ì¶œ (IN/OUT)
    
    Parameters:
    -----------
    soup : BeautifulSoup
        íŒŒì‹±ëœ HTML
    target_address : str
        í™•ì¸í•  ëŒ€ìƒ ì£¼ì†Œ
    
    Returns:
    --------
    Optional[str] : 'IN', 'OUT', ë˜ëŠ” None
    """
    try:
        target_address = target_address.lower()
        
        # From/To ì£¼ì†Œ ì°¾ê¸°
        from_address = None
        to_address = None
        
        rows = soup.find_all('div', class_='row')
        
        for row in rows:
            text = row.get_text()
            
            if 'From:' in text:
                addr_link = row.find('a', href=re.compile(r'/address/0x'))
                if addr_link:
                    from_address = addr_link.get('href').split('/address/')[-1].lower()
            
            if 'To:' in text:
                addr_link = row.find('a', href=re.compile(r'/address/0x'))
                if addr_link:
                    to_address = addr_link.get('href').split('/address/')[-1].lower()
        
        # ë°©í–¥ íŒë‹¨
        if to_address and target_address in to_address:
            return 'IN'
        elif from_address and target_address in from_address:
            return 'OUT'
        
        return None
    
    except Exception as e:
        print(f"âš ï¸ Direction ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None


def extract_value_and_usd(soup: BeautifulSoup) -> Tuple[Optional[str], Optional[str]]:
    """
    ê±°ë˜ ê¸ˆì•¡ê³¼ USD ê°€ì¹˜ ì¶”ì¶œ
    
    Returns:
    --------
    Tuple[Optional[str], Optional[str]] : (amount_text, amount_usd)
    """
    try:
        # Value ì„¹ì…˜ ì°¾ê¸°
        rows = soup.find_all('div', class_='row')
        
        for row in rows:
            if 'Value:' in row.get_text():
                # BNB ê¸ˆì•¡
                amount_span = row.find('span', class_='u-label--value')
                if amount_span:
                    amount_text = clean_text(amount_span.get_text())
                    
                    # USD ê°€ì¹˜ (tooltip ë˜ëŠ” ë³„ë„ í‘œì‹œ)
                    usd_match = re.search(r'\$([0-9,]+\.[0-9]+)', row.get_text())
                    if usd_match:
                        amount_usd = usd_match.group(1).replace(',', '')
                        return amount_text, amount_usd
                    
                    return amount_text, None
        
        return None, None
    
    except Exception as e:
        print(f"âš ï¸ Value/USD ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        return None, None


def scrape_transaction_details(
    tx_hash: str,
    target_address: Optional[str] = None,
    session: Optional[requests.Session] = None
) -> Dict:
    """
    íŠ¹ì • ê±°ë˜ì˜ ìƒì„¸ ì •ë³´ë¥¼ ì›¹ ìŠ¤í¬ë˜í•‘ìœ¼ë¡œ ìˆ˜ì§‘
    
    Parameters:
    -----------
    tx_hash : str
        ê±°ë˜ í•´ì‹œ
    target_address : Optional[str]
        Direction íŒë‹¨ì„ ìœ„í•œ ëŒ€ìƒ ì£¼ì†Œ
    session : Optional[requests.Session]
        ì¬ì‚¬ìš©í•  requests ì„¸ì…˜
    
    Returns:
    --------
    Dict : ì¶”ê°€ ì •ë³´ ë”•ì…”ë„ˆë¦¬
        {
            'input_data': str,  # Method
            'from_label': str,
            'to_label': str,
            'direction': str,  # IN/OUT
            'amount_text': str,
            'amount_usd': str
        }
    """
    url = f"{BSCSCAN_BASE_URL}/tx/{tx_hash}"
    
    # ì„¸ì…˜ì´ ì—†ìœ¼ë©´ ìƒˆë¡œ ìƒì„±
    if session is None:
        session = requests.Session()
        close_session = True
    else:
        close_session = False
    
    result = {
        'input_data': None,
        'from_label': None,
        'to_label': None,
        'direction': None,
        'amount_text': None,
        'amount_usd': None
    }
    
    for attempt in range(MAX_RETRIES):
        try:
            response = session.get(url, headers=get_headers(), timeout=REQUEST_TIMEOUT)
            response.raise_for_status()
            
            # Rate limiting ì²´í¬
            if response.status_code == 429:
                wait_time = (attempt + 1) * RETRY_DELAY
                print(f"âš ï¸ Rate limit ë„ë‹¬. {wait_time}ì´ˆ ëŒ€ê¸°...")
                time.sleep(wait_time)
                continue
            
            # HTML íŒŒì‹±
            soup = BeautifulSoup(response.text, 'html.parser')
            
            # ì •ë³´ ì¶”ì¶œ
            result['input_data'] = extract_method(soup)
            result['from_label'], result['to_label'] = extract_labels(soup)
            
            if target_address:
                result['direction'] = extract_direction(soup, target_address)
            
            result['amount_text'], result['amount_usd'] = extract_value_and_usd(soup)
            
            # ì„±ê³µ
            return result
        
        except requests.exceptions.RequestException as e:
            print(f"âš ï¸ ìŠ¤í¬ë˜í•‘ ì‹¤íŒ¨ (ì‹œë„ {attempt + 1}/{MAX_RETRIES}): {e}")
            
            if attempt < MAX_RETRIES - 1:
                wait_time = (attempt + 1) * RETRY_DELAY
                time.sleep(wait_time)
            else:
                print(f"âŒ ê±°ë˜ {tx_hash[:10]}... ìŠ¤í¬ë˜í•‘ ìµœì¢… ì‹¤íŒ¨")
        
        except Exception as e:
            print(f"âŒ ì˜ˆìƒì¹˜ ëª»í•œ ì˜¤ë¥˜: {e}")
            break
    
    if close_session:
        session.close()
    
    return result


def scrape_multiple_transactions(
    transactions: list,
    delay: float = RETRY_DELAY
) -> list:
    """
    ì—¬ëŸ¬ ê±°ë˜ì˜ ìƒì„¸ ì •ë³´ë¥¼ ìˆœì°¨ì ìœ¼ë¡œ ìŠ¤í¬ë˜í•‘
    
    Parameters:
    -----------
    transactions : list
        ê±°ë˜ ë¦¬ìŠ¤íŠ¸ (ê° í•­ëª©ì€ 'tx_hash' í•„ë“œ í•„ìš”)
    delay : float
        ìš”ì²­ ê°„ ëŒ€ê¸° ì‹œê°„ (ì´ˆ)
    
    Returns:
    --------
    list : ì¶”ê°€ ì •ë³´ê°€ í¬í•¨ëœ ê±°ë˜ ë¦¬ìŠ¤íŠ¸
    """
    print(f"\nğŸŒ ì›¹ ìŠ¤í¬ë˜í•‘ ì‹œì‘: {len(transactions)}ê±´")
    print(f"ì˜ˆìƒ ì†Œìš” ì‹œê°„: ì•½ {len(transactions) * delay / 60:.1f}ë¶„")
    
    # ì¬ì‚¬ìš©í•  ì„¸ì…˜ ìƒì„±
    session = requests.Session()
    
    enriched_transactions = []
    success_count = 0
    
    try:
        for i, tx in enumerate(transactions, 1):
            tx_hash = tx.get('tx_hash')
            target_address = tx.get('from_address') or tx.get('to_address')
            
            if not tx_hash:
                print(f"âš ï¸ [{i}/{len(transactions)}] tx_hash ì—†ìŒ, ê±´ë„ˆëœ€")
                enriched_transactions.append(tx)
                continue
            
            print(f"[{i}/{len(transactions)}] {tx_hash[:10]}... ìŠ¤í¬ë˜í•‘ ì¤‘...")
            
            # ìŠ¤í¬ë˜í•‘ ì‹¤í–‰
            additional_info = scrape_transaction_details(
                tx_hash, 
                target_address=target_address,
                session=session
            )
            
            # ê¸°ì¡´ ì •ë³´ì— ì¶”ê°€ ì •ë³´ ë³‘í•©
            enriched_tx = tx.copy()
            
            # input_data ì—…ë°ì´íŠ¸ (Method)
            if additional_info.get('input_data'):
                enriched_tx['input_data'] = additional_info['input_data']
            
            # Label ì—…ë°ì´íŠ¸
            if additional_info.get('from_label'):
                enriched_tx['from_label'] = additional_info['from_label']
            
            if additional_info.get('to_label'):
                enriched_tx['to_label'] = additional_info['to_label']
            
            # USD ê°€ì¹˜ ì—…ë°ì´íŠ¸
            if additional_info.get('amount_usd'):
                try:
                    enriched_tx['amount_usd'] = float(additional_info['amount_usd'])
                except:
                    pass
            
            enriched_transactions.append(enriched_tx)
            success_count += 1
            
            # ì§„í–‰ ìƒí™©
            if i % 10 == 0:
                print(f"  ì§„í–‰ë¥ : {i}/{len(transactions)} ({i/len(transactions)*100:.1f}%)")
                print(f"  ì„±ê³µ: {success_count}ê±´")
            
            # Rate limiting
            time.sleep(delay)
    
    except KeyboardInterrupt:
        print("\nâš ï¸ ì‚¬ìš©ìì— ì˜í•´ ì¤‘ë‹¨ë˜ì—ˆìŠµë‹ˆë‹¤.")
        print(f"ì²˜ë¦¬ ì™„ë£Œ: {len(enriched_transactions)}/{len(transactions)}ê±´")
    
    finally:
        session.close()
    
    print(f"\nâœ… ì›¹ ìŠ¤í¬ë˜í•‘ ì™„ë£Œ: {success_count}/{len(transactions)}ê±´ ì„±ê³µ")
    
    return enriched_transactions


def main():
    """ë©”ì¸ ì‹¤í–‰ í•¨ìˆ˜ (í…ŒìŠ¤íŠ¸ìš©)"""
    import argparse
    
    parser = argparse.ArgumentParser(description='BSC Web Scraper')
    parser.add_argument('--tx-hash', type=str, help='í…ŒìŠ¤íŠ¸í•  ê±°ë˜ í•´ì‹œ')
    parser.add_argument('--address', type=str, help='Direction íŒë‹¨ìš© ì£¼ì†Œ')
    args = parser.parse_args()
    
    if args.tx_hash:
        print(f"ğŸ§ª í…ŒìŠ¤íŠ¸: {args.tx_hash}")
        result = scrape_transaction_details(args.tx_hash, args.address)
        
        print(f"\nğŸ“Š ê²°ê³¼:")
        for key, value in result.items():
            print(f"  {key}: {value}")
    else:
        print("ì‚¬ìš©ë²•: python bsc_web_scraper.py --tx-hash 0x...")
        print("ì˜ˆì‹œ: python bsc_web_scraper.py --tx-hash 0x1234... --address 0xabcd...")


if __name__ == '__main__':
    main()

