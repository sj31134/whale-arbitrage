#!/usr/bin/env python3
"""
CoinCarpì—ì„œ BNB ì½”ì¸ì˜ Rich List Top 100 ê³ ë˜ ì§€ê°‘ ì£¼ì†Œë¥¼ ì¶”ì¶œí•˜ì—¬ CSVë¡œ ì €ì¥
"""

import requests
from bs4 import BeautifulSoup
import re
import csv
import time
from pathlib import Path

def extract_bnb_richlist():
    """BNB ì½”ì¸ì˜ Rich List Top 100 ì¶”ì¶œ"""
    url = 'https://www.coincarp.com/ko/currencies/binance-coin/richlist/'
    headers = {
        'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
        'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
        'Accept-Language': 'ko-KR,ko;q=0.9,en-US;q=0.8,en;q=0.7'
    }
    
    print("=" * 70)
    print("BNB ì½”ì¸ Rich List Top 100 ì¶”ì¶œ")
    print("=" * 70)
    print(f"\nURL: {url}")
    print("í˜ì´ì§€ ìš”ì²­ ì¤‘...")
    
    try:
        response = requests.get(url, headers=headers, timeout=30)
        response.raise_for_status()
        print(f"âœ… ì‘ë‹µ ìƒíƒœ: {response.status_code}")
        
        soup = BeautifulSoup(response.content, 'html.parser')
        
        # ë°©ë²• 1: data-copy-text ì†ì„±ì—ì„œ ì§€ê°‘ ì£¼ì†Œ ì¶”ì¶œ
        copy_elements = soup.find_all(attrs={'data-copy-text': True})
        addresses_from_attr = []
        
        for elem in copy_elements:
            addr = elem.get('data-copy-text', '').strip()
            if addr and addr.startswith('0x') and len(addr) == 42:
                addresses_from_attr.append(addr)
        
        # ì¤‘ë³µ ì œê±°
        addresses_from_attr = list(dict.fromkeys(addresses_from_attr))  # ìˆœì„œ ìœ ì§€í•˜ë©´ì„œ ì¤‘ë³µ ì œê±°
        
        print(f"\n[ë°©ë²• 1] data-copy-text ì†ì„±ì—ì„œ ì¶”ì¶œ: {len(addresses_from_attr)}ê°œ")
        
        # ë°©ë²• 2: ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì§€ê°‘ ì£¼ì†Œ ì¶”ì¶œ
        address_pattern = r'0x[a-fA-F0-9]{40}'
        addresses_from_regex = re.findall(address_pattern, response.text)
        addresses_from_regex = list(dict.fromkeys(addresses_from_regex))
        
        print(f"[ë°©ë²• 2] ì •ê·œí‘œí˜„ì‹ìœ¼ë¡œ ì¶”ì¶œ: {len(addresses_from_regex)}ê°œ")
        
        # ë°©ë²• 3: í…Œì´ë¸” í–‰ì—ì„œ ì§ì ‘ ì¶”ì¶œ
        rows = soup.find_all('tr')
        addresses_from_table = []
        
        for row in rows:
            # td2 í´ë˜ìŠ¤ë¥¼ ê°€ì§„ ì…€ì—ì„œ ì£¼ì†Œ ì°¾ê¸°
            td2 = row.find('td', class_='td2')
            if td2:
                # address-item í´ë˜ìŠ¤ ë‚´ì˜ spanì—ì„œ ì£¼ì†Œ ì°¾ê¸°
                address_item = td2.find('div', class_='address-item')
                if address_item:
                    span = address_item.find('span', class_='mr-2')
                    if span:
                        addr = span.get_text(strip=True)
                        if addr and addr.startswith('0x') and len(addr) == 42:
                            addresses_from_table.append(addr)
        
        addresses_from_table = list(dict.fromkeys(addresses_from_table))
        print(f"[ë°©ë²• 3] í…Œì´ë¸”ì—ì„œ ì§ì ‘ ì¶”ì¶œ: {len(addresses_from_table)}ê°œ")
        
        # ëª¨ë“  ë°©ë²•ì—ì„œ ì£¼ì†Œ ìˆ˜ì§‘ (ì¤‘ë³µ ì œê±° ë° ì†Œë¬¸ì ì •ê·œí™”)
        all_addresses = set()
        
        for addr in addresses_from_attr + addresses_from_regex + addresses_from_table:
            if addr and addr.startswith('0x') and len(addr) == 42:
                all_addresses.add(addr.lower())  # ì†Œë¬¸ìë¡œ ì •ê·œí™”í•˜ì—¬ ì¤‘ë³µ ì œê±°
        
        # ë¦¬ìŠ¤íŠ¸ë¡œ ë³€í™˜ (ìˆœì„œ ìœ ì§€)
        selected_addresses = list(all_addresses)[:100]  # Top 100ë§Œ ì„ íƒ
        
        print(f"\nâœ… ìµœì¢… ì¶”ì¶œëœ ê³ ìœ  ì£¼ì†Œ: {len(selected_addresses)}ê°œ")
        
        if not selected_addresses:
            print("\nâš ï¸  ì§€ê°‘ ì£¼ì†Œë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            print("\nHTML êµ¬ì¡° ë¶„ì„ ì¤‘...")
            
            # ë””ë²„ê¹…: í…Œì´ë¸” êµ¬ì¡° í™•ì¸
            table = soup.find('table')
            if table:
                print(f"í…Œì´ë¸” ë°œê²¬: {len(table.find_all('tr'))}ê°œ í–‰")
            else:
                print("í…Œì´ë¸”ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # ìŠ¤í¬ë¦½íŠ¸ íƒœê·¸ì—ì„œ ë°ì´í„° ì°¾ê¸°
            scripts = soup.find_all('script')
            print(f"\nìŠ¤í¬ë¦½íŠ¸ íƒœê·¸: {len(scripts)}ê°œ")
            for i, script in enumerate(scripts[:3]):
                if script.string:
                    if 'address' in script.string.lower() or '0x' in script.string:
                        print(f"\nìŠ¤í¬ë¦½íŠ¸ {i+1} ë‚´ìš© (ì²˜ìŒ 500ì):")
                        print(script.string[:500])
            
            return None
        
        return selected_addresses
        
    except Exception as e:
        print(f"\nâŒ ì˜¤ë¥˜ ë°œìƒ: {e}")
        import traceback
        traceback.print_exc()
        return None

def save_to_csv(addresses, filename='bnb_richlist_top100.csv'):
    """ì§€ê°‘ ì£¼ì†Œë¥¼ CSV íŒŒì¼ë¡œ ì €ì¥"""
    if not addresses:
        print("ì €ì¥í•  ì£¼ì†Œê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    output_path = Path(__file__).parent / filename
    
    with open(output_path, 'w', newline='', encoding='utf-8') as f:
        writer = csv.writer(f)
        writer.writerow(['rank', 'address', 'chain_type'])
        
        for rank, address in enumerate(addresses, 1):
            writer.writerow([rank, address, 'BSC'])
    
    print(f"\nâœ… CSV íŒŒì¼ ì €ì¥ ì™„ë£Œ: {output_path}")
    print(f"   ì´ {len(addresses)}ê°œì˜ ì§€ê°‘ ì£¼ì†Œ ì €ì¥ë¨")
    
    return output_path

def main():
    """ë©”ì¸ í•¨ìˆ˜"""
    addresses = extract_bnb_richlist()
    
    if addresses:
        output_path = save_to_csv(addresses)
        
        print("\n" + "=" * 70)
        print("âœ… ì‘ì—… ì™„ë£Œ")
        print("=" * 70)
        print(f"\nğŸ“Š ê²°ê³¼:")
        print(f"   - ì¶”ì¶œëœ ì£¼ì†Œ ìˆ˜: {len(addresses)}ê°œ")
        print(f"   - ì €ì¥ íŒŒì¼: {output_path}")
        print(f"\nìƒ˜í”Œ ì£¼ì†Œ (ìµœëŒ€ 5ê°œ):")
        for i, addr in enumerate(addresses[:5], 1):
            print(f"   {i}. {addr}")
    else:
        print("\nâŒ ì£¼ì†Œ ì¶”ì¶œ ì‹¤íŒ¨")

if __name__ == '__main__':
    main()

