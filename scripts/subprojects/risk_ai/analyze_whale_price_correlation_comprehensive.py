#!/usr/bin/env python3
"""
ê³ ë˜ ë°ì´í„° ê¸°ë°˜ ê°€ê²© ìƒê´€ì„± ë¶„ì„ (ì¢…í•©)
- ë…ë¦½ë³€ìˆ˜: ê³ ë˜ êµ¬ë§¤/íŒë§¤/ì´ë™, ê±°ë˜ì†Œ ìˆœìœ ì…/ìœ ì¶œ, ë³€ë™ì„±, íŒŒìƒ ì„ ë¬¼ ê±°ë˜ëŸ‰
- ì¢…ì†ë³€ìˆ˜: BTC/ETH ê°€ê²© (ì •ì , ë³€í™”ìœ¨, ë³€ë™ì„±, ë°©í–¥)
"""

import sys
from pathlib import Path
import pandas as pd
import numpy as np
from datetime import datetime, timedelta
import argparse
import logging
from scipy import stats
from sklearn.linear_model import LinearRegression

try:
    from statsmodels.stats.multitest import multipletests
    HAS_STATSMODELS = True
except ImportError:
    HAS_STATSMODELS = False

ROOT = Path(__file__).resolve().parents[3]
sys.path.insert(0, str(ROOT))
sys.path.insert(0, str(ROOT / "scripts" / "subprojects" / "risk_ai"))

from feature_engineering import FeatureEngineer
import sqlite3

DB_PATH = ROOT / "data" / "project.db"
OUTPUT_DIR = ROOT / "data" / "analysis"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)


def load_whale_data(coin: str = "BTC", start_date: str = "2022-01-01") -> pd.DataFrame:
    """ê³ ë˜ ë°ì´í„° ë¡œë“œ"""
    conn = sqlite3.connect(DB_PATH)
    
    # whale_daily_stats ë¡œë“œ
    query_whale = f"""
    SELECT 
        date,
        coin_symbol,
        exchange_inflow_usd,
        exchange_outflow_usd,
        net_flow_usd,
        whale_to_whale_usd,
        active_addresses,
        large_tx_count,
        avg_tx_size_usd
    FROM whale_daily_stats
    WHERE coin_symbol = '{coin}'
    AND date >= '{start_date}'
    ORDER BY date
    """
    
    whale_df = pd.read_sql(query_whale, conn)
    
    # whale_transactionsì—ì„œ ê±°ë˜ ë°©í–¥ë³„ ì§‘ê³„
    query_tx = f"""
    SELECT 
        DATE(block_timestamp) as date,
        coin_symbol,
        transaction_direction,
        SUM(amount_usd) as total_amount_usd,
        COUNT(*) as tx_count
    FROM whale_transactions
    WHERE coin_symbol = '{coin}'
    AND block_timestamp >= '{start_date}'
    GROUP BY DATE(block_timestamp), coin_symbol, transaction_direction
    """
    
    try:
        tx_df = pd.read_sql(query_tx, conn)
        
        # ê±°ë˜ ë°©í–¥ë³„ í”¼ë²—
        if not tx_df.empty:
            tx_pivot = tx_df.pivot_table(
                index=['date', 'coin_symbol'],
                columns='transaction_direction',
                values=['total_amount_usd', 'tx_count'],
                aggfunc='sum',
                fill_value=0
            ).reset_index()
            
            # ì»¬ëŸ¼ëª… ì •ë¦¬
            tx_pivot.columns = ['_'.join(col).strip('_') if col[1] else col[0] for col in tx_pivot.columns]
            
            # whale_dfì™€ ë³‘í•©
            whale_df = pd.merge(
                whale_df,
                tx_pivot,
                on=['date', 'coin_symbol'],
                how='left'
            )
    except Exception as e:
        logging.warning(f"whale_transactions ì¡°íšŒ ì‹¤íŒ¨: {e}")
    
    conn.close()
    
    if whale_df.empty:
        return pd.DataFrame()
    
    whale_df['date'] = pd.to_datetime(whale_df['date'])
    
    return whale_df


def load_derivatives_data(coin: str = "BTC", start_date: str = "2022-01-01") -> pd.DataFrame:
    """íŒŒìƒìƒí’ˆ ë°ì´í„° ë¡œë“œ"""
    conn = sqlite3.connect(DB_PATH)
    
    symbol = f"{coin}USDT"
    
    query = f"""
    SELECT 
        f.date,
        f.symbol,
        f.avg_funding_rate,
        f.sum_open_interest,
        f.long_short_ratio,
        f.volatility_24h,
        e.taker_buy_sell_ratio,
        e.taker_buy_vol,
        e.taker_sell_vol,
        e.top_trader_long_short_ratio,
        e.bybit_funding_rate,
        e.bybit_oi
    FROM binance_futures_metrics f
    LEFT JOIN futures_extended_metrics e ON f.date = e.date AND f.symbol = e.symbol
    WHERE f.symbol = '{symbol}'
    AND f.date >= '{start_date}'
    ORDER BY f.date
    """
    
    df = pd.read_sql(query, conn)
    conn.close()
    
    if df.empty:
        return pd.DataFrame()
    
    df['date'] = pd.to_datetime(df['date'])
    
    # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
    numeric_cols = [
        'avg_funding_rate', 'sum_open_interest', 'long_short_ratio',
        'volatility_24h', 'taker_buy_sell_ratio', 'taker_buy_vol',
        'taker_sell_vol', 'top_trader_long_short_ratio',
        'bybit_funding_rate', 'bybit_oi'
    ]
    
    for col in numeric_cols:
        if col in df.columns:
            df[col] = pd.to_numeric(df[col], errors='coerce')
    
    return df


def load_price_data(coin: str = "BTC", start_date: str = "2022-01-01") -> pd.DataFrame:
    """ê°€ê²© ë°ì´í„° ë¡œë“œ"""
    conn = sqlite3.connect(DB_PATH)
    
    symbol = f"{coin}USDT"
    
    query = f"""
    SELECT 
        date,
        close,
        high,
        low,
        volume
    FROM binance_spot_daily
    WHERE symbol = '{symbol}'
    AND date >= '{start_date}'
    ORDER BY date
    """
    
    df = pd.read_sql(query, conn)
    conn.close()
    
    if df.empty:
        return pd.DataFrame()
    
    df['date'] = pd.to_datetime(df['date'])
    df['close'] = pd.to_numeric(df['close'], errors='coerce')
    df['high'] = pd.to_numeric(df['high'], errors='coerce')
    df['low'] = pd.to_numeric(df['low'], errors='coerce')
    
    return df


def calculate_target_variables(price_df: pd.DataFrame) -> pd.DataFrame:
    """ì¢…ì†ë³€ìˆ˜ ê³„ì‚°"""
    df = price_df.copy()
    
    # ê°€ê²© ë³€í™”ìœ¨
    df['price_change_1d'] = df['close'].pct_change(1).shift(-1)  # ë‹¤ìŒë‚  ê°€ê²© ë³€í™”ìœ¨
    df['price_change_7d'] = df['close'].pct_change(7).shift(-7)  # 7ì¼ í›„ ê°€ê²© ë³€í™”ìœ¨
    df['price_change_30d'] = df['close'].pct_change(30).shift(-30)  # 30ì¼ í›„ ê°€ê²© ë³€í™”ìœ¨
    
    # ë³€ë™ì„±
    df['volatility_1d'] = df['close'].pct_change(1).rolling(window=5).std().shift(-1)
    df['volatility_7d'] = df['close'].pct_change(1).rolling(window=7).std().shift(-7)
    
    # ê°€ê²© ë°©í–¥
    df['price_direction_1d'] = (df['close'].diff().shift(-1) > 0).astype(int)
    df['price_direction_7d'] = (df['close'].diff(7).shift(-7) > 0).astype(int)
    
    # ê°€ê²© ì •ì  ê°’
    df['price_static'] = df['close']
    
    # ê°€ê²© ë²”ìœ„
    df['price_range'] = (df['high'] - df['low']) / df['close']
    
    return df


def main():
    parser = argparse.ArgumentParser(description="ê³ ë˜ ë°ì´í„° ê¸°ë°˜ ê°€ê²© ìƒê´€ì„± ë¶„ì„ (ì¢…í•©)")
    parser.add_argument("--coin", type=str, default="BTC", help="ë¶„ì„í•  ì½”ì¸ ì‹¬ë³¼ (ì˜ˆ: BTC, ETH)")
    parser.add_argument("--start-date", type=str, default="2022-01-01", help="ë¶„ì„ ì‹œì‘ì¼ (YYYY-MM-DD)")
    args = parser.parse_args()
    
    logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")
    
    print("=" * 80)
    print("ğŸ“Š ê³ ë˜ ë°ì´í„° ê¸°ë°˜ ê°€ê²© ìƒê´€ì„± ë¶„ì„ (ì¢…í•©)")
    print("=" * 80)
    print(f"ì½”ì¸: {args.coin}")
    print(f"ì‹œì‘ì¼: {args.start_date}")
    
    # 1. ë°ì´í„° ë¡œë“œ
    print("\n[1/5] ë°ì´í„° ë¡œë“œ ì¤‘...")
    
    price_df = load_price_data(args.coin, args.start_date)
    if price_df.empty:
        logging.error("âŒ ê°€ê²© ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    logging.info(f"   âœ… ê°€ê²© ë°ì´í„°: {len(price_df)}ê±´")
    
    whale_df = load_whale_data(args.coin, args.start_date)
    if whale_df.empty:
        logging.warning("âš ï¸ ê³ ë˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        whale_df = pd.DataFrame()
    else:
        logging.info(f"   âœ… ê³ ë˜ ë°ì´í„°: {len(whale_df)}ê±´")
    
    derivatives_df = load_derivatives_data(args.coin, args.start_date)
    if derivatives_df.empty:
        logging.warning("âš ï¸ íŒŒìƒìƒí’ˆ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        derivatives_df = pd.DataFrame()
    else:
        logging.info(f"   âœ… íŒŒìƒìƒí’ˆ ë°ì´í„°: {len(derivatives_df)}ê±´")
    
    # 2. ì¢…ì†ë³€ìˆ˜ ê³„ì‚°
    print("\n[2/5] ì¢…ì†ë³€ìˆ˜ ê³„ì‚° ì¤‘...")
    price_df = calculate_target_variables(price_df)
    logging.info("   âœ… ì¢…ì†ë³€ìˆ˜ ê³„ì‚° ì™„ë£Œ")
    
    # 3. ë°ì´í„° ë³‘í•©
    print("\n[3/5] ë°ì´í„° ë³‘í•© ì¤‘...")
    merged_df = price_df[['date', 'close', 'price_change_1d', 'price_change_7d', 
                          'price_change_30d', 'volatility_1d', 'volatility_7d',
                          'price_direction_1d', 'price_direction_7d', 'price_static', 'price_range']].copy()
    
    if not whale_df.empty:
        merged_df = pd.merge(merged_df, whale_df, on='date', how='inner')
    
    if not derivatives_df.empty:
        merged_df = pd.merge(merged_df, derivatives_df[['date', 'avg_funding_rate', 'sum_open_interest',
                                                         'long_short_ratio', 'volatility_24h',
                                                         'taker_buy_sell_ratio', 'taker_buy_vol',
                                                         'taker_sell_vol', 'top_trader_long_short_ratio',
                                                         'bybit_funding_rate', 'bybit_oi']],
                            on='date', how='inner')
    
    merged_df = merged_df.dropna(subset=['close'])
    
    if merged_df.empty:
        logging.error("âŒ ë³‘í•©ëœ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    logging.info(f"   âœ… ë³‘í•© ì™„ë£Œ: {len(merged_df)}ê±´")
    
    # 4. ë…ë¦½ë³€ìˆ˜ ì •ì˜
    print("\n[4/5] ìƒê´€ê´€ê³„ ê³„ì‚° ì¤‘...")
    
    # ë…ë¦½ë³€ìˆ˜ ëª©ë¡
    independent_vars = []
    
    # ê³ ë˜ ê±°ë˜ ë°ì´í„°
    if not whale_df.empty:
        independent_vars.extend([
            'exchange_inflow_usd',      # ê±°ë˜ì†Œ ìœ ì…
            'exchange_outflow_usd',     # ê±°ë˜ì†Œ ìœ ì¶œ
            'net_flow_usd',             # ìˆœìœ ì…
            'whale_to_whale_usd',       # ê³ ë˜ê°„ ê±°ë˜
            'active_addresses',         # í™œì„± ì£¼ì†Œ ìˆ˜
            'large_tx_count',           # ëŒ€í˜• ê±°ë˜ ê±´ìˆ˜
            'avg_tx_size_usd'           # í‰ê·  ê±°ë˜ í¬ê¸°
        ])
        
        # ê±°ë˜ ë°©í–¥ë³„ ë³€ìˆ˜ (ìˆëŠ” ê²½ìš°)
        tx_direction_cols = [col for col in merged_df.columns if 'total_amount_usd' in col or 'tx_count' in col]
        independent_vars.extend(tx_direction_cols)
    
    # íŒŒìƒìƒí’ˆ ë°ì´í„°
    if not derivatives_df.empty:
        independent_vars.extend([
            'avg_funding_rate',         # í€ë”©ë¹„
            'sum_open_interest',        # ë¯¸ê²°ì œì•½ì •
            'long_short_ratio',        # ë¡±/ìˆ ë¹„ìœ¨
            'volatility_24h',          # ë³€ë™ì„±
            'taker_buy_sell_ratio',    # Taker ë§¤ìˆ˜/ë§¤ë„ ë¹„ìœ¨
            'taker_buy_vol',           # Taker ë§¤ìˆ˜ ê±°ë˜ëŸ‰
            'taker_sell_vol',          # Taker ë§¤ë„ ê±°ë˜ëŸ‰
            'top_trader_long_short_ratio',  # íƒ‘ íŠ¸ë ˆì´ë” ë¡±/ìˆ ë¹„ìœ¨
            'bybit_funding_rate',      # Bybit í€ë”©ë¹„
            'bybit_oi'                 # Bybit OI
        ])
    
    # ì¡´ì¬í•˜ëŠ” ë³€ìˆ˜ë§Œ ì‚¬ìš©
    available_vars = [v for v in independent_vars if v in merged_df.columns]
    
    # ì¢…ì†ë³€ìˆ˜ ëª©ë¡
    target_vars = [
        'price_static',           # í˜„ì¬ ê°€ê²©
        'price_change_1d',        # 1ì¼ í›„ ê°€ê²© ë³€í™”ìœ¨
        'price_change_7d',        # 7ì¼ í›„ ê°€ê²© ë³€í™”ìœ¨
        'price_change_30d',       # 30ì¼ í›„ ê°€ê²© ë³€í™”ìœ¨
        'volatility_1d',          # 1ì¼ í›„ ë³€ë™ì„±
        'volatility_7d',          # 7ì¼ í›„ ë³€ë™ì„±
        'price_direction_1d',     # 1ì¼ í›„ ê°€ê²© ë°©í–¥
        'price_direction_7d',     # 7ì¼ í›„ ê°€ê²© ë°©í–¥
        'price_range'             # ê°€ê²© ë²”ìœ„
    ]
    
    # 5. ìƒê´€ê´€ê³„ ê³„ì‚°
    correlation_results = []
    
    for var in available_vars:
        for target in target_vars:
            for lag in [0, 1, 3, 7, 14]:  # 0ì¼, 1ì¼, 3ì¼, 7ì¼, 14ì¼ í›„
                if var in merged_df.columns and target in merged_df.columns:
                    x = merged_df[var].shift(lag).dropna()
                    y = merged_df[target].loc[x.index].dropna()
                    
                    if len(x) > 1 and len(y) > 1 and len(x) == len(y):
                        # NaN ì œê±°
                        valid_mask = ~(pd.isna(x) | pd.isna(y))
                        x_clean = x[valid_mask]
                        y_clean = y[valid_mask]
                        
                        if len(x_clean) > 1:
                            try:
                                pearson_r, pearson_p = stats.pearsonr(x_clean, y_clean)
                                spearman_r, spearman_p = stats.spearmanr(x_clean, y_clean)
                                
                                correlation_results.append({
                                    'variable': var,
                                    'target': target,
                                    'lag': lag,
                                    'pearson_correlation': pearson_r,
                                    'pearson_pvalue': pearson_p,
                                    'spearman_correlation': spearman_r,
                                    'spearman_pvalue': spearman_p,
                                    'sample_size': len(x_clean)
                                })
                            except Exception as e:
                                logging.warning(f"      âš ï¸ ìƒê´€ê´€ê³„ ê³„ì‚° ì‹¤íŒ¨ ({var} vs {target}, lag {lag}): {e}")
    
    if not correlation_results:
        logging.error("âŒ ìƒê´€ê´€ê³„ ê²°ê³¼ê°€ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    corr_df = pd.DataFrame(correlation_results)
    
    # FDR ë³´ì •
    if HAS_STATSMODELS:
        try:
            pvalues = corr_df['pearson_pvalue'].values
            reject, pvals_corrected, _, _ = multipletests(pvalues, alpha=0.05, method='fdr_bh')
            corr_df['pearson_pvalue_fdr'] = pvals_corrected
            corr_df['is_significant_fdr'] = reject
            logging.info("   âœ… FDR ë³´ì • ì ìš© ì™„ë£Œ")
        except Exception as e:
            logging.warning(f"   âš ï¸ FDR ë³´ì • ì‹¤íŒ¨: {e}")
            corr_df['pearson_pvalue_fdr'] = corr_df['pearson_pvalue']
            corr_df['is_significant_fdr'] = corr_df['pearson_pvalue'] < 0.05
    else:
        logging.warning("   âš ï¸ statsmodelsê°€ ì—†ì–´ FDR ë³´ì •ì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        corr_df['pearson_pvalue_fdr'] = corr_df['pearson_pvalue']
        corr_df['is_significant_fdr'] = corr_df['pearson_pvalue'] < 0.05
    
    # ì„ í˜• íšŒê·€ ë¶„ì„
    print("\n[5/5] ì„ í˜• íšŒê·€ ë¶„ì„ ì¤‘...")
    regression_results = []
    significant_corr_df = corr_df[corr_df['is_significant_fdr']].copy()
    
    if not significant_corr_df.empty:
        for idx, row in significant_corr_df.iterrows():
            var = row['variable']
            target = row['target']
            lag = row['lag']
            
            x = merged_df[var].shift(lag).dropna()
            y = merged_df[target].loc[x.index].dropna()
            
            valid_mask = ~(pd.isna(x) | pd.isna(y))
            x_clean = x[valid_mask]
            y_clean = y[valid_mask]
            
            if len(x_clean) > 1:
                try:
                    model = LinearRegression()
                    model.fit(x_clean.values.reshape(-1, 1), y_clean.values)
                    r_squared = model.score(x_clean.values.reshape(-1, 1), y_clean.values)
                    regression_results.append({
                        'variable': var,
                        'target': target,
                        'lag': lag,
                        'r_squared': r_squared,
                        'coefficient': model.coef_[0],
                        'intercept': model.intercept_
                    })
                except Exception as e:
                    logging.warning(f"      âš ï¸ íšŒê·€ ë¶„ì„ ì‹¤íŒ¨ ({var} vs {target}, lag {lag}): {e}")
        
        if regression_results:
            reg_df = pd.DataFrame(regression_results)
            corr_df = pd.merge(corr_df, reg_df, on=['variable', 'target', 'lag'], how='left')
            logging.info(f"   âœ… {len(reg_df)}ê°œ íšŒê·€ ë¶„ì„ ì™„ë£Œ")
    
    # ê²°ê³¼ ì €ì¥
    output_path = OUTPUT_DIR / f"whale_price_correlation_comprehensive_{args.coin}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    corr_df.to_csv(output_path, index=False)
    logging.info(f"âœ… ê²°ê³¼ ì €ì¥: {output_path}")
    
    # ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š ìƒìœ„ 20ê°œ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„")
    print("=" * 80)
    significant = corr_df[corr_df['is_significant_fdr']].sort_values('pearson_correlation', ascending=False, key=abs)
    print(significant.head(20).to_string())
    
    print("\n" + "=" * 80)
    print("âœ… ë¶„ì„ ì™„ë£Œ")
    print("=" * 80)
    print(f"ì´ ë¶„ì„ ì¡°í•©: {len(corr_df)}ê°œ")
    print(f"ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„: {corr_df['is_significant_fdr'].sum()}ê°œ (p < 0.05, FDR ë³´ì •)")
    print("=" * 80)


if __name__ == "__main__":
    main()

