#!/usr/bin/env python3
"""
Project 3: í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ëª¨ë¸ í›ˆë ¨

XGBoost (ì •ì /ë™ì  ë³€ìˆ˜) + LSTM (ì‹œê³„ì—´ íŒ¨í„´) + Meta Model (ê²°í•©)
"""

import os
import sys
import json
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime

# TensorFlow ê²½ê³  ë©”ì‹œì§€ ì–µì œ
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

import xgboost as xgb
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score

ROOT = Path(__file__).resolve().parents[3]
sys.path.insert(0, str(ROOT / "scripts" / "subprojects" / "risk_ai"))

from feature_engineering import FeatureEngineer

# LSTM ëª¨ë¸ ì„í¬íŠ¸ (TensorFlow ìˆì„ ë•Œë§Œ)
try:
    from train_lstm_model import LSTMRiskModel, HAS_TENSORFLOW
except ImportError:
    HAS_TENSORFLOW = False
    LSTMRiskModel = None

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ
MODEL_DIR = ROOT / "data" / "models"
MODEL_DIR.mkdir(parents=True, exist_ok=True)


class HybridEnsembleModel:
    """
    í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ëª¨ë¸
    
    êµ¬ì¡°:
    1. XGBoost: ì •ì  + ë™ì  ë³€ìˆ˜ ì²˜ë¦¬ (í…Œì´ë¸” í˜•íƒœ ë°ì´í„°ì— ê°•ì )
    2. LSTM: ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµ (ì‹œê°„ì  ì˜ì¡´ì„± ìº¡ì²˜)
    3. Meta Model (Logistic Regression): ë‘ ëª¨ë¸ì˜ ì˜ˆì¸¡ê°’ ê²°í•©
    """
    
    def __init__(self, sequence_length=30, use_lstm=True):
        """
        Args:
            sequence_length: LSTM ì‹œí€€ìŠ¤ ê¸¸ì´
            use_lstm: LSTM ëª¨ë¸ ì‚¬ìš© ì—¬ë¶€ (TensorFlow ì—†ìœ¼ë©´ ìë™ ë¹„í™œì„±í™”)
        """
        self.sequence_length = sequence_length
        self.use_lstm = use_lstm and HAS_TENSORFLOW
        
        # ëª¨ë¸ ì»´í¬ë„ŒíŠ¸
        self.xgb_model = None
        self.lstm_model = None
        self.meta_model = None
        
        # ìŠ¤ì¼€ì¼ëŸ¬
        self.xgb_scaler = StandardScaler()
        self.lstm_scaler = StandardScaler()
        
        # ë©”íƒ€ë°ì´í„°
        self.feature_names = None
        self.n_features = None
        
    def build_xgb_model(self):
        """XGBoost ëª¨ë¸ ìƒì„±"""
        self.xgb_model = xgb.XGBClassifier(
            n_estimators=200,
            max_depth=6,
            learning_rate=0.05,
            subsample=0.8,
            colsample_bytree=0.8,
            min_child_weight=3,
            gamma=0.1,
            reg_alpha=0.1,
            reg_lambda=1.0,
            scale_pos_weight=2,  # ë¶ˆê· í˜• ë°ì´í„° ì²˜ë¦¬
            random_state=42,
            use_label_encoder=False,
            eval_metric='auc'
        )
        return self.xgb_model
    
    def build_lstm_model(self, n_features):
        """LSTM ëª¨ë¸ ìƒì„±"""
        if not self.use_lstm:
            return None
        
        self.lstm_model = LSTMRiskModel(sequence_length=self.sequence_length)
        self.lstm_model.build_model(n_features)
        return self.lstm_model
    
    def build_meta_model(self):
        """ë©”íƒ€ ëª¨ë¸ ìƒì„± (Logistic Regression)"""
        self.meta_model = LogisticRegression(
            C=1.0,
            class_weight='balanced',
            random_state=42,
            max_iter=1000
        )
        return self.meta_model
    
    def fit(self, X_train, y_train, X_val=None, y_val=None, verbose=1):
        """
        ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ
        
        Args:
            X_train: í•™ìŠµ íŠ¹ì„±
            y_train: í•™ìŠµ íƒ€ê²Ÿ
            X_val: ê²€ì¦ íŠ¹ì„± (ì˜µì…˜)
            y_val: ê²€ì¦ íƒ€ê²Ÿ (ì˜µì…˜)
            verbose: ì¶œë ¥ ë ˆë²¨
        """
        self.n_features = X_train.shape[1]
        
        # ============================================
        # 1. XGBoost í•™ìŠµ
        # ============================================
        if verbose:
            print("\n[1/3] XGBoost ëª¨ë¸ í•™ìŠµ ì¤‘...")
        
        X_train_xgb = self.xgb_scaler.fit_transform(X_train)
        
        self.build_xgb_model()
        
        eval_set = [(X_train_xgb, y_train)]
        if X_val is not None and y_val is not None:
            X_val_xgb = self.xgb_scaler.transform(X_val)
            eval_set.append((X_val_xgb, y_val))
        
        self.xgb_model.fit(
            X_train_xgb, y_train,
            eval_set=eval_set,
            verbose=False
        )
        
        xgb_train_pred = self.xgb_model.predict_proba(X_train_xgb)[:, 1]
        if verbose:
            train_auc = roc_auc_score(y_train, xgb_train_pred) if len(np.unique(y_train)) > 1 else 0
            print(f"   XGBoost Train AUC: {train_auc:.4f}")
        
        # ============================================
        # 2. LSTM í•™ìŠµ (ì„ íƒì )
        # ============================================
        lstm_train_pred = None
        
        if self.use_lstm:
            if verbose:
                print("\n[2/3] LSTM ëª¨ë¸ í•™ìŠµ ì¤‘...")
            
            self.build_lstm_model(self.n_features)
            
            # ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (í•™ìŠµ ë°ì´í„°ì˜ 20%)
            val_split = int(len(X_train) * 0.8)
            X_train_lstm = X_train[:val_split]
            y_train_lstm = y_train[:val_split]
            X_val_lstm = X_train[val_split:]
            y_val_lstm = y_train[val_split:]
            
            self.lstm_model.fit(
                X_train_lstm, y_train_lstm,
                X_val=X_val_lstm, y_val=y_val_lstm,
                epochs=50,
                batch_size=32,
                verbose=0
            )
            
            # LSTM ì˜ˆì¸¡ (ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ì•ë¶€ë¶„ ì œì™¸)
            lstm_train_pred_full = self.lstm_model.predict(X_train)
            
            # íŒ¨ë”© (ì‹œí€€ìŠ¤ ê¸¸ì´ë§Œí¼ ì•ë¶€ë¶„ì€ 0ìœ¼ë¡œ)
            lstm_train_pred = np.zeros(len(X_train))
            lstm_train_pred[self.sequence_length:] = lstm_train_pred_full
            
            if verbose:
                valid_idx = self.sequence_length
                train_auc = roc_auc_score(y_train[valid_idx:], lstm_train_pred[valid_idx:]) if len(np.unique(y_train[valid_idx:])) > 1 else 0
                print(f"   LSTM Train AUC: {train_auc:.4f}")
        else:
            if verbose:
                print("\n[2/3] LSTM ê±´ë„ˆëœ€ (TensorFlow ë¯¸ì„¤ì¹˜)")
        
        # ============================================
        # 3. Meta Model í•™ìŠµ
        # ============================================
        if verbose:
            print("\n[3/3] Meta Model í•™ìŠµ ì¤‘...")
        
        self.build_meta_model()
        
        # ë©”íƒ€ íŠ¹ì„± ìƒì„±
        if self.use_lstm:
            meta_features = np.column_stack([xgb_train_pred, lstm_train_pred])
        else:
            meta_features = xgb_train_pred.reshape(-1, 1)
        
        self.meta_model.fit(meta_features, y_train)
        
        meta_train_pred = self.meta_model.predict_proba(meta_features)[:, 1]
        if verbose:
            train_auc = roc_auc_score(y_train, meta_train_pred) if len(np.unique(y_train)) > 1 else 0
            print(f"   Meta Model Train AUC: {train_auc:.4f}")
        
        return self
    
    def predict_proba(self, X):
        """í™•ë¥  ì˜ˆì¸¡"""
        # XGBoost ì˜ˆì¸¡
        # - Streamlit Cloud ë“± ì¼ë¶€ í™˜ê²½ì—ì„œ xgboost sklearn wrapper(XGBClassifier)ì™€ scikit-learn ì¡°í•©ì´
        #   _estimator_type ê´€ë ¨ í˜¸í™˜ ë¬¸ì œë¥¼ ì¼ìœ¼í‚¤ëŠ” ì‚¬ë¡€ê°€ ìˆì–´,
        #   ê°€ëŠ¥í•˜ë©´ Booster ê¸°ë°˜ ì˜ˆì¸¡ìœ¼ë¡œ ìš°íšŒí•©ë‹ˆë‹¤.
        X_xgb = self.xgb_scaler.transform(X)
        try:
            # Boosterì¼ ê²½ìš° (ê¶Œì¥)
            if isinstance(self.xgb_model, xgb.Booster):
                dmat = xgb.DMatrix(X_xgb, feature_names=self.feature_names)
                xgb_pred = self.xgb_model.predict(dmat)
            else:
                # sklearn wrapperì¼ ê²½ìš°
                xgb_pred = self.xgb_model.predict_proba(X_xgb)[:, 1]
        except Exception as e:
            # ìµœí›„ì˜ í´ë°±: Boosterë¡œ ê°•ì œ ì‹œë„
            try:
                booster = getattr(self.xgb_model, "get_booster", None)
                if callable(booster):
                    dmat = xgb.DMatrix(X_xgb, feature_names=self.feature_names)
                    xgb_pred = booster().predict(dmat)
                else:
                    raise
            except Exception as e2:
                raise RuntimeError(f"XGBoost ì˜ˆì¸¡ ì‹¤íŒ¨: {e} / Booster í´ë°± ì‹¤íŒ¨: {e2}") from e2
        
        # LSTM ì˜ˆì¸¡
        if self.use_lstm and self.lstm_model is not None:
            lstm_pred_full = self.lstm_model.predict(X)
            lstm_pred = np.zeros(len(X))
            lstm_pred[self.sequence_length:] = lstm_pred_full
            
            meta_features = np.column_stack([xgb_pred, lstm_pred])
        else:
            meta_features = xgb_pred.reshape(-1, 1)
        
        # Meta Model ì˜ˆì¸¡
        proba = self.meta_model.predict_proba(meta_features)
        
        return proba
    
    def predict(self, X):
        """í´ë˜ìŠ¤ ì˜ˆì¸¡"""
        proba = self.predict_proba(X)
        return (proba[:, 1] > 0.5).astype(int)
    
    def evaluate(self, X_test, y_test):
        """ëª¨ë¸ í‰ê°€"""
        y_pred_proba = self.predict_proba(X_test)[:, 1]
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        metrics = {
            'accuracy': accuracy_score(y_test, y_pred),
            'precision': precision_score(y_test, y_pred, zero_division=0),
            'recall': recall_score(y_test, y_pred, zero_division=0),
            'f1': f1_score(y_test, y_pred, zero_division=0),
            'auc_roc': roc_auc_score(y_test, y_pred_proba) if len(np.unique(y_test)) > 1 else 0
        }
        
        return metrics
    
    def get_component_predictions(self, X):
        """ê° ì»´í¬ë„ŒíŠ¸ ëª¨ë¸ì˜ ê°œë³„ ì˜ˆì¸¡ê°’ ë°˜í™˜"""
        X_xgb = self.xgb_scaler.transform(X)
        xgb_pred = self.xgb_model.predict_proba(X_xgb)[:, 1]
        
        result = {'xgb': xgb_pred}
        
        if self.use_lstm and self.lstm_model is not None:
            lstm_pred_full = self.lstm_model.predict(X)
            lstm_pred = np.zeros(len(X))
            lstm_pred[self.sequence_length:] = lstm_pred_full
            result['lstm'] = lstm_pred
        
        result['ensemble'] = self.predict_proba(X)[:, 1]
        
        return result
    
    def save(self, model_name="hybrid_ensemble_model"):
        """ëª¨ë¸ ì €ì¥"""
        # XGBoost ì €ì¥
        xgb_path = MODEL_DIR / f"{model_name}_xgb.json"
        self.xgb_model.save_model(xgb_path)
        
        # XGBoost ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        xgb_scaler_path = MODEL_DIR / f"{model_name}_xgb_scaler.pkl"
        with open(xgb_scaler_path, 'wb') as f:
            pickle.dump(self.xgb_scaler, f)
        
        # LSTM ì €ì¥ (ìˆìœ¼ë©´)
        if self.use_lstm and self.lstm_model is not None:
            self.lstm_model.save(f"{model_name}_lstm")
        
        # Meta Model ì €ì¥
        meta_path = MODEL_DIR / f"{model_name}_meta.pkl"
        with open(meta_path, 'wb') as f:
            pickle.dump(self.meta_model, f)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'sequence_length': self.sequence_length,
            'use_lstm': self.use_lstm,
            'n_features': self.n_features,
            'feature_names': self.feature_names,
            'created_at': datetime.now().isoformat()
        }
        metadata_path = MODEL_DIR / f"{model_name}_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"âœ… ì•™ìƒë¸” ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {MODEL_DIR / model_name}")
    
    def load(self, model_name="hybrid_ensemble_model"):
        """ëª¨ë¸ ë¡œë“œ"""
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        metadata_path = MODEL_DIR / f"{model_name}_metadata.json"
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        self.sequence_length = metadata['sequence_length']
        self.use_lstm = metadata['use_lstm']
        self.n_features = metadata['n_features']
        self.feature_names = metadata.get('feature_names')
        
        # XGBoost ë¡œë“œ
        xgb_path = MODEL_DIR / f"{model_name}_xgb.json"
        # Streamlit Cloud ë“±ì—ì„œ sklearn wrapper í˜¸í™˜ ë¬¸ì œê°€ ìˆì„ ìˆ˜ ìˆì–´ Boosterë¡œ ë¡œë“œ (ê¶Œì¥)
        try:
            self.xgb_model = xgb.Booster()
            self.xgb_model.load_model(xgb_path)
        except Exception:
            # í´ë°±: sklearn wrapperë¡œ ë¡œë“œ
            self.xgb_model = xgb.XGBClassifier()
            self.xgb_model.load_model(xgb_path)
        
        # XGBoost ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        xgb_scaler_path = MODEL_DIR / f"{model_name}_xgb_scaler.pkl"
        with open(xgb_scaler_path, 'rb') as f:
            self.xgb_scaler = pickle.load(f)
        
        # LSTM ë¡œë“œ (ìˆìœ¼ë©´)
        if self.use_lstm and HAS_TENSORFLOW:
            self.lstm_model = LSTMRiskModel(sequence_length=self.sequence_length)
            try:
                self.lstm_model.load(f"{model_name}_lstm")
            except FileNotFoundError:
                print("âš ï¸ LSTM ëª¨ë¸ íŒŒì¼ ì—†ìŒ, LSTM ë¹„í™œì„±í™”")
                self.use_lstm = False
        
        # Meta Model ë¡œë“œ
        meta_path = MODEL_DIR / f"{model_name}_meta.pkl"
        with open(meta_path, 'rb') as f:
            self.meta_model = pickle.load(f)
        
        print(f"âœ… ì•™ìƒë¸” ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_name}")


def train_hybrid_model(include_dynamic=True, use_lstm=True):
    """í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ"""
    
    print("=" * 80)
    print("ğŸ“Š í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ")
    print("=" * 80)
    
    # ë°ì´í„° ì¤€ë¹„
    print("\n[1/4] ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    fe = FeatureEngineer()
    train_df, test_df, feature_cols = fe.prepare_ml_dataset(include_dynamic=include_dynamic)
    
    print(f"   í•™ìŠµ ë°ì´í„°: {len(train_df)}ì¼")
    print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ì¼")
    print(f"   íŠ¹ì„± ìˆ˜: {len(feature_cols)}")
    print(f"   ë™ì  ë³€ìˆ˜ í¬í•¨: {include_dynamic}")
    print(f"   LSTM ì‚¬ìš©: {use_lstm and HAS_TENSORFLOW}")
    
    # íŠ¹ì„± ë° íƒ€ê²Ÿ ë¶„ë¦¬
    X_train = train_df[feature_cols].values
    y_train = train_df['target_high_vol'].values
    X_test = test_df[feature_cols].values
    y_test = test_df['target_high_vol'].values
    
    # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    print("\n[2/4] ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì¤‘...")
    model = HybridEnsembleModel(sequence_length=30, use_lstm=use_lstm)
    model.feature_names = feature_cols
    
    model.fit(X_train, y_train, verbose=1)
    
    # í‰ê°€
    print("\n[3/4] ëª¨ë¸ í‰ê°€ ì¤‘...")
    metrics = model.evaluate(X_test, y_test)
    
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   Accuracy: {metrics['accuracy']:.4f}")
    print(f"   Precision: {metrics['precision']:.4f}")
    print(f"   Recall: {metrics['recall']:.4f}")
    print(f"   F1 Score: {metrics['f1']:.4f}")
    print(f"   AUC-ROC: {metrics['auc_roc']:.4f}")
    
    # ì»´í¬ë„ŒíŠ¸ë³„ ì„±ëŠ¥ ë¹„êµ
    print("\nğŸ“Š ì»´í¬ë„ŒíŠ¸ë³„ ì˜ˆì¸¡ ë¹„êµ:")
    component_preds = model.get_component_predictions(X_test)
    
    for name, pred in component_preds.items():
        if len(np.unique(y_test)) > 1:
            auc = roc_auc_score(y_test, pred)
            print(f"   {name.upper()}: AUC = {auc:.4f}")
    
    # ëª¨ë¸ ì €ì¥
    print("\n[4/4] ëª¨ë¸ ì €ì¥ ì¤‘...")
    model_name = "hybrid_ensemble_dynamic" if include_dynamic else "hybrid_ensemble_static"
    model.save(model_name)
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'model_name': model_name,
        'include_dynamic': include_dynamic,
        'use_lstm': use_lstm and HAS_TENSORFLOW,
        'n_features': len(feature_cols),
        'train_samples': len(train_df),
        'test_samples': len(test_df),
        'metrics': metrics,
        'trained_at': datetime.now().isoformat()
    }
    
    results_path = MODEL_DIR / f"{model_name}_results.json"
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: {results_path}")
    print("\n" + "=" * 80)
    print("âœ… í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print("=" * 80)
    
    return model, metrics


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description="í•˜ì´ë¸Œë¦¬ë“œ ì•™ìƒë¸” ëª¨ë¸ í•™ìŠµ")
    parser.add_argument("--no-dynamic", action="store_true", 
                        help="ë™ì  ë³€ìˆ˜ ì œì™¸")
    parser.add_argument("--no-lstm", action="store_true",
                        help="LSTM ëª¨ë¸ ì œì™¸ (XGBoostë§Œ ì‚¬ìš©)")
    
    args = parser.parse_args()
    
    train_hybrid_model(
        include_dynamic=not args.no_dynamic,
        use_lstm=not args.no_lstm
    )


if __name__ == "__main__":
    main()

