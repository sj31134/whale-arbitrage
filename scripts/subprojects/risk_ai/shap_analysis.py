#!/usr/bin/env python3
"""
Project 3: Risk AI SHAP Analysis
íŠ¹ì„± ê¸°ì—¬ë„ ë¶„ì„ ë° ì‹œê°í™”
"""

import pandas as pd
import numpy as np
from pathlib import Path
from feature_engineering import FeatureEngineer
from lightgbm import LGBMClassifier
from sklearn.metrics import roc_auc_score

ROOT = Path(__file__).resolve().parents[3]

try:
    import shap
    SHAP_AVAILABLE = True
except ImportError:
    SHAP_AVAILABLE = False
    print("âš ï¸ SHAPê°€ ì„¤ì¹˜ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
    print("   ì„¤ì¹˜: pip install shap")

def main():
    if not SHAP_AVAILABLE:
        print("SHAP ë¶„ì„ì„ ìˆ˜í–‰í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print("ğŸ” Project 3: Risk AI SHAP Analysis")
    print("=" * 80)
    
    fe = FeatureEngineer()
    
    # 1. ë°ì´í„° ì¤€ë¹„
    print("\nğŸ“Š ë°ì´í„°ì…‹ ìƒì„± ë° ì „ì²˜ë¦¬ ì¤‘...")
    train_df, test_df, features = fe.prepare_ml_dataset()
    
    X_train = train_df[features]
    y_train = train_df['target_high_vol']
    X_test = test_df[features]
    y_test = test_df['target_high_vol']
    
    # 2. ëª¨ë¸ í•™ìŠµ
    print("\nğŸ¤– ëª¨ë¸ í•™ìŠµ ì¤‘...")
    model = LGBMClassifier(
        n_estimators=200,
        learning_rate=0.05,
        max_depth=5,
        random_state=42,
        class_weight='balanced',
        verbosity=-1
    )
    
    model.fit(X_train, y_train)
    
    # 3. SHAP ê°’ ê³„ì‚°
    print("\nğŸ“Š SHAP ê°’ ê³„ì‚° ì¤‘...")
    print("   (ì‹œê°„ì´ ê±¸ë¦´ ìˆ˜ ìˆìŠµë‹ˆë‹¤)")
    
    # TreeExplainer ì‚¬ìš© (LightGBMì— ìµœì í™”)
    explainer = shap.TreeExplainer(model)
    
    # í…ŒìŠ¤íŠ¸ ì„¸íŠ¸ ìƒ˜í”Œë§ (ì „ì²´ ê³„ì‚°ì€ ì‹œê°„ì´ ì˜¤ë˜ ê±¸ë¦¼)
    sample_size = min(100, len(X_test))
    X_test_sample = X_test.sample(n=sample_size, random_state=42)
    y_test_sample = y_test.loc[X_test_sample.index]
    
    print(f"   ìƒ˜í”Œ í¬ê¸°: {sample_size}ê±´")
    shap_values = explainer.shap_values(X_test_sample)
    
    # ì´ì§„ ë¶„ë¥˜ì˜ ê²½ìš° shap_valuesëŠ” ë¦¬ìŠ¤íŠ¸ [class_0, class_1]
    if isinstance(shap_values, list):
        shap_values = shap_values[1]  # class_1 (ê³ ë³€ë™ì„±)ì— ëŒ€í•œ SHAP ê°’
    
    # 4. SHAP ìš”ì•½ í†µê³„
    print("\nğŸ“ˆ SHAP ìš”ì•½ í†µê³„")
    print("=" * 80)
    
    # íŠ¹ì„±ë³„ í‰ê·  ì ˆëŒ€ SHAP ê°’ (ì¤‘ìš”ë„)
    shap_importance = pd.DataFrame({
        'feature': features,
        'mean_abs_shap': np.abs(shap_values).mean(axis=0),
        'std_shap': shap_values.std(axis=0)
    }).sort_values('mean_abs_shap', ascending=False)
    
    print("\níŠ¹ì„±ë³„ í‰ê·  ì ˆëŒ€ SHAP ê°’ (ì¤‘ìš”ë„):")
    print(shap_importance.to_string(index=False))
    
    # 5. SHAP ê°’ ì €ì¥
    output_dir = ROOT / "data" / "project3_shap"
    output_dir.mkdir(parents=True, exist_ok=True)
    
    # SHAP ê°’ DataFrame
    shap_df = pd.DataFrame(
        shap_values,
        columns=features,
        index=X_test_sample.index
    )
    shap_df['predicted_prob'] = model.predict_proba(X_test_sample)[:, 1]
    shap_df['actual_label'] = y_test_sample.values
    
    shap_df.to_csv(output_dir / "shap_values.csv")
    print(f"\nğŸ’¾ SHAP ê°’ ì €ì¥ ì™„ë£Œ: {output_dir / 'shap_values.csv'}")
    
    # 6. íŠ¹ì„±ë³„ ê¸°ì—¬ë„ ë¶„ì„
    print("\nğŸ“Š íŠ¹ì„±ë³„ ê¸°ì—¬ë„ ë¶„ì„")
    print("=" * 80)
    
    for feature in features:
        feature_shap = shap_values[:, features.index(feature)]
        positive_contrib = (feature_shap > 0).sum()
        negative_contrib = (feature_shap < 0).sum()
        
        print(f"\n{feature}:")
        print(f"  í‰ê·  SHAP: {feature_shap.mean():.6f}")
        print(f"  í‘œì¤€í¸ì°¨: {feature_shap.std():.6f}")
        print(f"  ì–‘ì˜ ê¸°ì—¬: {positive_contrib}ê±´ ({positive_contrib/len(feature_shap)*100:.1f}%)")
        print(f"  ìŒì˜ ê¸°ì—¬: {negative_contrib}ê±´ ({negative_contrib/len(feature_shap)*100:.1f}%)")
    
    # 7. ê°œë³„ ì˜ˆì¸¡ í•´ì„ (ìƒ˜í”Œ)
    print("\nğŸ” ê°œë³„ ì˜ˆì¸¡ í•´ì„ (ìƒ˜í”Œ 3ê±´)")
    print("=" * 80)
    
    # ê³ ë³€ë™ì„± ì˜ˆì¸¡ í™•ë¥ ì´ ë†’ì€ ìƒ˜í”Œ
    high_prob_idx = shap_df.nlargest(3, 'predicted_prob').index
    
    for idx in high_prob_idx:
        print(f"\nìƒ˜í”Œ {idx}:")
        print(f"  ì˜ˆì¸¡ í™•ë¥ : {shap_df.loc[idx, 'predicted_prob']:.4f}")
        print(f"  ì‹¤ì œ ë ˆì´ë¸”: {shap_df.loc[idx, 'actual_label']}")
        
        sample_shap = shap_values[shap_df.index.get_loc(idx)]
        top_contributors = pd.DataFrame({
            'feature': features,
            'shap_value': sample_shap
        }).sort_values('shap_value', ascending=False, key=abs)
        
        print(f"  ì£¼ìš” ê¸°ì—¬ íŠ¹ì„± (ìƒìœ„ 3ê°œ):")
        for _, row in top_contributors.head(3).iterrows():
            direction = "ì¦ê°€" if row['shap_value'] > 0 else "ê°ì†Œ"
            print(f"    - {row['feature']}: {row['shap_value']:.6f} ({direction})")
    
    print("\n" + "=" * 80)
    print("âœ… SHAP ë¶„ì„ ì™„ë£Œ!")
    print("=" * 80)
    print(f"\nğŸ’¡ ì‹œê°í™”ë¥¼ ì›í•˜ì‹œë©´ ë‹¤ìŒ ëª…ë ¹ì„ ì‹¤í–‰í•˜ì„¸ìš”:")
    print(f"   shap.summary_plot(shap_values, X_test_sample, feature_names=features)")
    print(f"   shap.waterfall_plot(explainer.expected_value[1], shap_values[0], X_test_sample.iloc[0], feature_names=features)")

if __name__ == "__main__":
    main()

