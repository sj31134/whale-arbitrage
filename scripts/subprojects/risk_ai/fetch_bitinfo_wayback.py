#!/usr/bin/env python3
"""
BitInfoCharts ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ (Wayback Machine í™œìš©)
- 2023-01-01ë¶€í„° í˜„ì¬ê¹Œì§€ì˜ Top 100 Richest List ìŠ¤ëƒ…ìƒ·ì„ Wayback Machineì—ì„œ ê°€ì ¸ì˜´
"""

import sqlite3
import time
import re
from datetime import datetime, timedelta
from pathlib import Path
import requests
from bs4 import BeautifulSoup
import cloudscraper

ROOT = Path(__file__).resolve().parents[3]
DB_PATH = ROOT / "data" / "project.db"

BITINFO_URLS = {
    "BTC": "https://bitinfocharts.com/top-100-richest-bitcoin-addresses.html",
    "ETH": "https://bitinfocharts.com/top-100-richest-ethereum-addresses.html",
}

WAYBACK_API = "https://web.archive.org/cdx/search/cdx"


def ensure_db():
    DB_PATH.parent.mkdir(parents=True, exist_ok=True)


def upsert_whale(date_str, coin, top100_pct, avg_tx_value):
    conn = sqlite3.connect(DB_PATH)
    cursor = conn.cursor()
    cursor.execute("""
        INSERT OR REPLACE INTO bitinfocharts_whale 
        (date, coin, top100_richest_pct, avg_transaction_value_btc)
        VALUES (?, ?, ?, ?)
    """, (date_str, coin, top100_pct, avg_tx_value))
    conn.commit()
    conn.close()


def get_wayback_snapshots(url, start_date, end_date):
    """Wayback Machineì—ì„œ íŠ¹ì • URLì˜ ìŠ¤ëƒ…ìƒ· ëª©ë¡ ê°€ì ¸ì˜¤ê¸°"""
    params = {
        "url": url,
        "from": start_date.strftime("%Y%m%d"),
        "to": end_date.strftime("%Y%m%d"),
        "output": "json",
        "collapse": "timestamp:8"  # ì¼ë³„ë¡œ í•˜ë‚˜ì”©ë§Œ
    }
    
    try:
        response = requests.get(WAYBACK_API, params=params, timeout=30)
        response.raise_for_status()
        data = response.json()
        
        # ì²« ë²ˆì§¸ í–‰ì€ í—¤ë”
        if len(data) <= 1:
            return []
        
        snapshots = []
        for row in data[1:]:  # í—¤ë” ì œì™¸
            if len(row) >= 2:
                timestamp = row[1]  # YYYYMMDDHHmmss
                wayback_url = f"https://web.archive.org/web/{timestamp}/{url}"
                snapshots.append({
                    "timestamp": timestamp,
                    "date": timestamp[:8],  # YYYYMMDD
                    "url": wayback_url
                })
        
        return snapshots
    except Exception as e:
        print(f"âš ï¸ Wayback API ì˜¤ë¥˜: {e}")
        return []


def parse_stats_from_html(html, coin):
    """HTMLì—ì„œ í†µê³„ íŒŒì‹±"""
    soup = BeautifulSoup(html, "lxml")
    text = soup.get_text(separator=" ")
    
    top100_pct = None
    avg_tx = None
    
    # Top 100 richest percentage
    pct_match = re.search(r"Top\s*100\s.*?([\d\.,]+)%", text)
    if pct_match:
        top100_pct = float(pct_match.group(1).replace(",", ""))
    
    # Average transaction value
    avg_match = re.search(r"Avg(?:\.|average)\s*transaction\s*(?:value|size)\s*[:\-]?\s*\$?([\d\.,]+)", text, re.IGNORECASE)
    if avg_match:
        avg_tx_value = avg_match.group(1).replace(",", "")
        try:
            avg_tx = float(avg_tx_value)
        except ValueError:
            pass
    
    return top100_pct, avg_tx


def fetch_snapshot(wayback_url):
    """Wayback Machine ìŠ¤ëƒ…ìƒ·ì—ì„œ ë°ì´í„° ì¶”ì¶œ"""
    scraper = cloudscraper.create_scraper()
    
    # HTTPSë¡œ ë³€ê²½
    if wayback_url.startswith("http://"):
        wayback_url = wayback_url.replace("http://", "https://", 1)
    
    max_retries = 3
    for attempt in range(max_retries):
        try:
            response = scraper.get(wayback_url, timeout=30)
            if response.status_code == 200:
                return response.text
            elif response.status_code == 404:
                # ìŠ¤ëƒ…ìƒ·ì´ ì—†ìŒ
                return None
        except Exception as e:
            if attempt < max_retries - 1:
                time.sleep(2 ** attempt)  # Exponential backoff
            else:
                print(f"  âš ï¸ ìŠ¤ëƒ…ìƒ· ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ (ì¬ì‹œë„ {attempt+1}/{max_retries}): {e}")
                return None
    return None


def collect_historical_data(coin, url):
    """ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘"""
    print(f"\nğŸ“Š {coin} ê³¼ê±° ë°ì´í„° ìˆ˜ì§‘ ì‹œì‘...")
    
    # 2023-01-01ë¶€í„° í˜„ì¬ê¹Œì§€
    start_date = datetime(2023, 1, 1)
    end_date = datetime.now()
    
    # Wayback Machineì—ì„œ ìŠ¤ëƒ…ìƒ· ëª©ë¡ ê°€ì ¸ì˜¤ê¸°
    print(f"  ğŸ” Wayback Machine ìŠ¤ëƒ…ìƒ· ê²€ìƒ‰ ì¤‘...")
    snapshots = get_wayback_snapshots(url, start_date, end_date)
    
    if not snapshots:
        print(f"  âš ï¸ {coin} ìŠ¤ëƒ…ìƒ· ì—†ìŒ")
        return
    
    print(f"  âœ… {len(snapshots)}ê°œ ìŠ¤ëƒ…ìƒ· ë°œê²¬")
    
    # ê° ìŠ¤ëƒ…ìƒ· ì²˜ë¦¬
    success_count = 0
    for i, snapshot in enumerate(snapshots[:100]):  # ìµœëŒ€ 100ê°œë§Œ (ë„ˆë¬´ ë§ìœ¼ë©´ ì‹œê°„ ì˜¤ë˜ ê±¸ë¦¼)
        date_str = snapshot["date"]
        wayback_url = snapshot["url"]
        
        # YYYYMMDD -> YYYY-MM-DD
        formatted_date = f"{date_str[:4]}-{date_str[4:6]}-{date_str[6:8]}"
        
        print(f"  [{i+1}/{min(len(snapshots), 100)}] {formatted_date} ì²˜ë¦¬ ì¤‘...", end=" ")
        
        html = fetch_snapshot(wayback_url)
        if html:
            top100_pct, avg_tx = parse_stats_from_html(html, coin)
            
            if top100_pct is not None or avg_tx is not None:
                upsert_whale(
                    formatted_date,
                    coin,
                    top100_pct or 0.0,
                    avg_tx or 0.0
                )
                success_count += 1
                print(f"âœ…")
            else:
                print(f"âš ï¸ íŒŒì‹± ì‹¤íŒ¨")
        else:
            print(f"âš ï¸ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨")
        
        time.sleep(0.5)  # Rate limit
    
    print(f"  âœ… {coin}: {success_count}ê±´ ìˆ˜ì§‘ ì™„ë£Œ")


def main():
    ensure_db()
    
    # BTC ìˆ˜ì§‘
    collect_historical_data("BTC", BITINFO_URLS["BTC"])
    
    # ETH ìˆ˜ì§‘ (500 ì—ëŸ¬ê°€ ë‚˜ë”ë¼ë„ Wayback Machineì€ ì‹œë„)
    collect_historical_data("ETH", BITINFO_URLS["ETH"])


if __name__ == "__main__":
    main()

