#!/usr/bin/env python3
"""
Project 3: LSTM ì‹œê³„ì—´ ëª¨ë¸ í›ˆë ¨

ë™ì  ë³€ìˆ˜ë¥¼ í™œìš©í•œ ì‹œê³„ì—´ íŒ¨í„´ í•™ìŠµì„ ìœ„í•œ LSTM ëª¨ë¸ êµ¬í˜„
- ì‹œí€€ìŠ¤ ë°ì´í„° ìƒì„±
- LSTM ëª¨ë¸ êµ¬ì¡°
- í•™ìŠµ ë° í‰ê°€
"""

import os
import sys
import json
import pickle
import numpy as np
import pandas as pd
from pathlib import Path
from datetime import datetime

# TensorFlow ê²½ê³  ë©”ì‹œì§€ ì–µì œ
os.environ['TF_CPP_MIN_LOG_LEVEL'] = '2'

try:
    import tensorflow as tf
    from tensorflow.keras.models import Sequential, Model, load_model
    from tensorflow.keras.layers import LSTM, Dense, Dropout, Input, BatchNormalization
    from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint, ReduceLROnPlateau
    from tensorflow.keras.optimizers import Adam
    HAS_TENSORFLOW = True
except ImportError:
    HAS_TENSORFLOW = False
    print("âš ï¸ TensorFlowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. pip install tensorflow ì‹¤í–‰ í•„ìš”")

from sklearn.preprocessing import StandardScaler
from sklearn.metrics import roc_auc_score, precision_score, recall_score, f1_score, accuracy_score

ROOT = Path(__file__).resolve().parents[3]
sys.path.insert(0, str(ROOT / "scripts" / "subprojects" / "risk_ai"))

from feature_engineering import FeatureEngineer

# ëª¨ë¸ ì €ì¥ ê²½ë¡œ
MODEL_DIR = ROOT / "data" / "models"
MODEL_DIR.mkdir(parents=True, exist_ok=True)


class LSTMRiskModel:
    """LSTM ê¸°ë°˜ ë¦¬ìŠ¤í¬ ì˜ˆì¸¡ ëª¨ë¸"""
    
    def __init__(self, sequence_length=30, n_features=None):
        """
        Args:
            sequence_length: ì…ë ¥ ì‹œí€€ìŠ¤ ê¸¸ì´ (ì¼ ìˆ˜)
            n_features: íŠ¹ì„± ìˆ˜ (ìë™ ì„¤ì • ê°€ëŠ¥)
        """
        self.sequence_length = sequence_length
        self.n_features = n_features
        self.model = None
        self.scaler = StandardScaler()
        self.feature_names = None
        self.history = None
        
    def build_model(self, n_features):
        """LSTM ëª¨ë¸ êµ¬ì¡° ì •ì˜"""
        if not HAS_TENSORFLOW:
            raise ImportError("TensorFlowê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        self.n_features = n_features
        
        model = Sequential([
            # ì²« ë²ˆì§¸ LSTM ë ˆì´ì–´
            LSTM(64, return_sequences=True, 
                 input_shape=(self.sequence_length, n_features)),
            BatchNormalization(),
            Dropout(0.2),
            
            # ë‘ ë²ˆì§¸ LSTM ë ˆì´ì–´
            LSTM(32, return_sequences=False),
            BatchNormalization(),
            Dropout(0.2),
            
            # Dense ë ˆì´ì–´
            Dense(16, activation='relu'),
            Dropout(0.1),
            
            # ì¶œë ¥ ë ˆì´ì–´ (ì´ì§„ ë¶„ë¥˜)
            Dense(1, activation='sigmoid')
        ])
        
        model.compile(
            optimizer=Adam(learning_rate=0.001),
            loss='binary_crossentropy',
            metrics=['accuracy', tf.keras.metrics.AUC(name='auc')]
        )
        
        self.model = model
        return model
    
    def create_sequences(self, X, y=None):
        """
        ì‹œê³„ì—´ ë°ì´í„°ë¥¼ LSTM ì…ë ¥ í˜•íƒœë¡œ ë³€í™˜
        
        Args:
            X: íŠ¹ì„± ë°ì´í„° (n_samples, n_features)
            y: íƒ€ê²Ÿ ë°ì´í„° (n_samples,)
        
        Returns:
            X_seq: (n_samples - sequence_length, sequence_length, n_features)
            y_seq: (n_samples - sequence_length,) if y is not None
        """
        X_seq = []
        y_seq = []
        
        for i in range(len(X) - self.sequence_length):
            X_seq.append(X[i:i + self.sequence_length])
            if y is not None:
                y_seq.append(y[i + self.sequence_length])
        
        X_seq = np.array(X_seq)
        
        if y is not None:
            y_seq = np.array(y_seq)
            return X_seq, y_seq
        
        return X_seq
    
    def fit(self, X_train, y_train, X_val=None, y_val=None, 
            epochs=100, batch_size=32, verbose=1):
        """
        ëª¨ë¸ í•™ìŠµ
        
        Args:
            X_train: í•™ìŠµ íŠ¹ì„± ë°ì´í„°
            y_train: í•™ìŠµ íƒ€ê²Ÿ ë°ì´í„°
            X_val: ê²€ì¦ íŠ¹ì„± ë°ì´í„°
            y_val: ê²€ì¦ íƒ€ê²Ÿ ë°ì´í„°
            epochs: í•™ìŠµ ì—í­ ìˆ˜
            batch_size: ë°°ì¹˜ í¬ê¸°
            verbose: ì¶œë ¥ ë ˆë²¨
        """
        # ìŠ¤ì¼€ì¼ë§
        X_train_scaled = self.scaler.fit_transform(X_train)
        
        # ì‹œí€€ìŠ¤ ìƒì„±
        X_train_seq, y_train_seq = self.create_sequences(X_train_scaled, y_train)
        
        # ëª¨ë¸ ë¹Œë“œ
        if self.model is None:
            self.build_model(X_train.shape[1])
        
        # ì½œë°± ì„¤ì •
        callbacks = [
            EarlyStopping(
                monitor='val_auc' if X_val is not None else 'auc',
                patience=10,
                mode='max',
                restore_best_weights=True
            ),
            ReduceLROnPlateau(
                monitor='val_loss' if X_val is not None else 'loss',
                factor=0.5,
                patience=5,
                min_lr=1e-6
            )
        ]
        
        # ê²€ì¦ ë°ì´í„° ì¤€ë¹„
        validation_data = None
        if X_val is not None and y_val is not None:
            X_val_scaled = self.scaler.transform(X_val)
            X_val_seq, y_val_seq = self.create_sequences(X_val_scaled, y_val)
            validation_data = (X_val_seq, y_val_seq)
        
        # í•™ìŠµ
        self.history = self.model.fit(
            X_train_seq, y_train_seq,
            validation_data=validation_data,
            epochs=epochs,
            batch_size=batch_size,
            callbacks=callbacks,
            verbose=verbose
        )
        
        return self.history
    
    def predict(self, X):
        """ì˜ˆì¸¡ ìˆ˜í–‰"""
        X_scaled = self.scaler.transform(X)
        X_seq = self.create_sequences(X_scaled)
        
        predictions = self.model.predict(X_seq, verbose=0)
        return predictions.flatten()
    
    def predict_proba(self, X):
        """í™•ë¥  ì˜ˆì¸¡ (sklearn í˜¸í™˜)"""
        probs = self.predict(X)
        return np.column_stack([1 - probs, probs])
    
    def evaluate(self, X_test, y_test):
        """ëª¨ë¸ í‰ê°€"""
        X_test_scaled = self.scaler.transform(X_test)
        X_test_seq, y_test_seq = self.create_sequences(X_test_scaled, y_test)
        
        # ì˜ˆì¸¡
        y_pred_proba = self.model.predict(X_test_seq, verbose=0).flatten()
        y_pred = (y_pred_proba > 0.5).astype(int)
        
        # ë©”íŠ¸ë¦­ ê³„ì‚°
        metrics = {
            'accuracy': accuracy_score(y_test_seq, y_pred),
            'precision': precision_score(y_test_seq, y_pred, zero_division=0),
            'recall': recall_score(y_test_seq, y_pred, zero_division=0),
            'f1': f1_score(y_test_seq, y_pred, zero_division=0),
            'auc_roc': roc_auc_score(y_test_seq, y_pred_proba) if len(np.unique(y_test_seq)) > 1 else 0
        }
        
        return metrics
    
    def save(self, model_name="lstm_risk_model"):
        """ëª¨ë¸ ì €ì¥"""
        # Keras ëª¨ë¸ ì €ì¥
        model_path = MODEL_DIR / f"{model_name}.keras"
        self.model.save(model_path)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ì €ì¥
        scaler_path = MODEL_DIR / f"{model_name}_scaler.pkl"
        with open(scaler_path, 'wb') as f:
            pickle.dump(self.scaler, f)
        
        # ë©”íƒ€ë°ì´í„° ì €ì¥
        metadata = {
            'sequence_length': self.sequence_length,
            'n_features': self.n_features,
            'feature_names': self.feature_names,
            'created_at': datetime.now().isoformat()
        }
        metadata_path = MODEL_DIR / f"{model_name}_metadata.json"
        with open(metadata_path, 'w') as f:
            json.dump(metadata, f, indent=2)
        
        print(f"âœ… ëª¨ë¸ ì €ì¥ ì™„ë£Œ: {model_path}")
    
    def load(self, model_name="lstm_risk_model"):
        """ëª¨ë¸ ë¡œë“œ"""
        model_path = MODEL_DIR / f"{model_name}.keras"
        scaler_path = MODEL_DIR / f"{model_name}_scaler.pkl"
        metadata_path = MODEL_DIR / f"{model_name}_metadata.json"
        
        if not model_path.exists():
            raise FileNotFoundError(f"ëª¨ë¸ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {model_path}")
        
        # ëª¨ë¸ ë¡œë“œ
        self.model = load_model(model_path)
        
        # ìŠ¤ì¼€ì¼ëŸ¬ ë¡œë“œ
        with open(scaler_path, 'rb') as f:
            self.scaler = pickle.load(f)
        
        # ë©”íƒ€ë°ì´í„° ë¡œë“œ
        with open(metadata_path, 'r') as f:
            metadata = json.load(f)
        
        self.sequence_length = metadata['sequence_length']
        self.n_features = metadata['n_features']
        self.feature_names = metadata.get('feature_names')
        
        print(f"âœ… ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")


def train_lstm_model(include_dynamic=True, sequence_length=30):
    """LSTM ëª¨ë¸ í•™ìŠµ ì‹¤í–‰"""
    
    if not HAS_TENSORFLOW:
        print("âŒ TensorFlowê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•„ LSTM í•™ìŠµì„ ê±´ë„ˆëœë‹ˆë‹¤.")
        return None
    
    print("=" * 80)
    print("ğŸ“Š LSTM ë¦¬ìŠ¤í¬ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
    print("=" * 80)
    
    # ë°ì´í„° ì¤€ë¹„
    print("\n[1/4] ë°ì´í„° ì¤€ë¹„ ì¤‘...")
    fe = FeatureEngineer()
    train_df, test_df, feature_cols = fe.prepare_ml_dataset(include_dynamic=include_dynamic)
    
    print(f"   í•™ìŠµ ë°ì´í„°: {len(train_df)}ì¼")
    print(f"   í…ŒìŠ¤íŠ¸ ë°ì´í„°: {len(test_df)}ì¼")
    print(f"   íŠ¹ì„± ìˆ˜: {len(feature_cols)}")
    print(f"   ë™ì  ë³€ìˆ˜ í¬í•¨: {include_dynamic}")
    
    # íŠ¹ì„± ë° íƒ€ê²Ÿ ë¶„ë¦¬
    X_train = train_df[feature_cols].values
    y_train = train_df['target_high_vol'].values
    X_test = test_df[feature_cols].values
    y_test = test_df['target_high_vol'].values
    
    # ëª¨ë¸ ìƒì„± ë° í•™ìŠµ
    print("\n[2/4] LSTM ëª¨ë¸ í•™ìŠµ ì¤‘...")
    model = LSTMRiskModel(sequence_length=sequence_length)
    model.feature_names = feature_cols
    
    # ê²€ì¦ ë°ì´í„° ë¶„ë¦¬ (í•™ìŠµ ë°ì´í„°ì˜ 20%)
    val_split = int(len(X_train) * 0.8)
    X_train_fit = X_train[:val_split]
    y_train_fit = y_train[:val_split]
    X_val = X_train[val_split:]
    y_val = y_train[val_split:]
    
    history = model.fit(
        X_train_fit, y_train_fit,
        X_val=X_val, y_val=y_val,
        epochs=100,
        batch_size=32,
        verbose=1
    )
    
    # í‰ê°€
    print("\n[3/4] ëª¨ë¸ í‰ê°€ ì¤‘...")
    metrics = model.evaluate(X_test, y_test)
    
    print("\nğŸ“Š í…ŒìŠ¤íŠ¸ ê²°ê³¼:")
    print(f"   Accuracy: {metrics['accuracy']:.4f}")
    print(f"   Precision: {metrics['precision']:.4f}")
    print(f"   Recall: {metrics['recall']:.4f}")
    print(f"   F1 Score: {metrics['f1']:.4f}")
    print(f"   AUC-ROC: {metrics['auc_roc']:.4f}")
    
    # ëª¨ë¸ ì €ì¥
    print("\n[4/4] ëª¨ë¸ ì €ì¥ ì¤‘...")
    model_name = "lstm_risk_model_dynamic" if include_dynamic else "lstm_risk_model_static"
    model.save(model_name)
    
    # ê²°ê³¼ ì €ì¥
    results = {
        'model_name': model_name,
        'include_dynamic': include_dynamic,
        'sequence_length': sequence_length,
        'n_features': len(feature_cols),
        'train_samples': len(train_df),
        'test_samples': len(test_df),
        'metrics': metrics,
        'trained_at': datetime.now().isoformat()
    }
    
    results_path = MODEL_DIR / f"{model_name}_results.json"
    with open(results_path, 'w') as f:
        json.dump(results, f, indent=2)
    
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: {results_path}")
    print("\n" + "=" * 80)
    print("âœ… LSTM ëª¨ë¸ í•™ìŠµ ì™„ë£Œ!")
    print("=" * 80)
    
    return model, metrics


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description="LSTM ë¦¬ìŠ¤í¬ ì˜ˆì¸¡ ëª¨ë¸ í•™ìŠµ")
    parser.add_argument("--no-dynamic", action="store_true", 
                        help="ë™ì  ë³€ìˆ˜ ì œì™¸")
    parser.add_argument("--sequence-length", type=int, default=30,
                        help="ì‹œí€€ìŠ¤ ê¸¸ì´ (ê¸°ë³¸ê°’: 30)")
    
    args = parser.parse_args()
    
    train_lstm_model(
        include_dynamic=not args.no_dynamic,
        sequence_length=args.sequence_length
    )


if __name__ == "__main__":
    main()

