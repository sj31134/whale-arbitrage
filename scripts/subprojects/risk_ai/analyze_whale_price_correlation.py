#!/usr/bin/env python3
"""
ê³ ë˜ ë°ì´í„°ì™€ ê°€ê²© ê°„ ì¢…í•© ìƒê´€ê´€ê³„ ë¶„ì„

ì¢…ì†ë³€ìˆ˜:
- price_static: ì •ì  ê°€ê²© (USD)
- price_change_1d, price_change_7d, price_change_30d: ê°€ê²© ë³€í™”ìœ¨
- volatility_1d, volatility_7d: ë³€ë™ì„±
- price_direction_1d, price_direction_7d: ê°€ê²© ë°©í–¥ (ìƒìŠ¹/í•˜ë½)

ë…ë¦½ë³€ìˆ˜ (ê³ ë˜/ì˜¨ì²´ì¸ ë³€ìˆ˜):
- ì˜¨ì²´ì¸ ë³€ìˆ˜: exchange_inflow_usd, exchange_outflow_usd, net_flow_usd,
              active_addresses, large_tx_count, avg_tx_size_usd
- íŒŒìƒìƒí’ˆ ë³€ìˆ˜: sum_open_interest, avg_funding_rate, long_short_ratio,
                taker_buy_sell_ratio, top_trader_long_short_ratio
- ê³ ë˜ ì§‘ì¤‘ë„ ë³€ìˆ˜: top100_richest_pct, whale_conc_change_7d

ë¶„ì„ ë°©ë²•:
1. ìƒê´€ê´€ê³„ ë¶„ì„ (Pearson, Spearman)
2. ê·¸ë ˆì¸ì € ì¸ê³¼ê´€ê³„ ê²€ì •
3. ì„ í˜• íšŒê·€ ë¶„ì„
4. ëœë¤ í¬ë ˆìŠ¤íŠ¸ í”¼ì²˜ ì¤‘ìš”ë„
5. SHAP ê°’ ë¶„ì„ (ìƒ˜í”Œë§)
"""

import os
import sys
import sqlite3
import pandas as pd
import numpy as np
from pathlib import Path
from datetime import datetime, timedelta
from scipy import stats
from typing import Dict, List, Tuple

ROOT = Path(__file__).resolve().parent.parents[2]
sys.path.insert(0, str(ROOT / "scripts" / "subprojects" / "risk_ai"))

from feature_engineering import FeatureEngineer

DB_PATH = ROOT / "data" / "project.db"
OUTPUT_DIR = ROOT / "data" / "analysis"
OUTPUT_DIR.mkdir(parents=True, exist_ok=True)

# ê·¸ë ˆì¸ì € ì¸ê³¼ê´€ê³„ ê²€ì • (ì„ íƒì )
try:
    from statsmodels.tsa.stattools import grangercausalitytests
    HAS_GRANGER = True
except ImportError:
    HAS_GRANGER = False

# ëœë¤ í¬ë ˆìŠ¤íŠ¸ (ì„ íƒì )
try:
    from sklearn.ensemble import RandomForestRegressor
    HAS_SKLEARN = True
except ImportError:
    HAS_SKLEARN = False

# SHAP (ì„ íƒì )
try:
    import shap
    HAS_SHAP = True
except ImportError:
    HAS_SHAP = False


def load_price_data(coin: str = "BTC", start_date: str = "2022-01-01") -> pd.DataFrame:
    """ê°€ê²© ë°ì´í„° ë¡œë“œ"""
    conn = sqlite3.connect(DB_PATH)
    
    symbol = f"{coin}USDT"
    
    query = f"""
    SELECT 
        date,
        close as price
    FROM binance_spot_daily
    WHERE symbol = '{symbol}'
    AND date >= '{start_date}'
    ORDER BY date
    """
    
    df = pd.read_sql(query, conn)
    conn.close()
    
    if len(df) == 0:
        return pd.DataFrame()
    
    df['date'] = pd.to_datetime(df['date'])
    df = df.sort_values('date').reset_index(drop=True)
    
    return df


def calculate_target_variables(price_df: pd.DataFrame) -> pd.DataFrame:
    """ì¢…ì†ë³€ìˆ˜ ê³„ì‚°"""
    df = price_df.copy()
    
    # ì •ì  ê°€ê²©
    df['price_static'] = df['price']
    
    # ê°€ê²© ë³€í™”ìœ¨
    df['price_change_1d'] = df['price'].pct_change(1).shift(-1)
    df['price_change_7d'] = df['price'].pct_change(7).shift(-7)
    df['price_change_30d'] = df['price'].pct_change(30).shift(-30)
    
    # ë³€ë™ì„±
    df['volatility_1d'] = df['price'].pct_change().rolling(1).std()
    df['volatility_7d'] = df['price'].pct_change().rolling(7).std()
    
    # ê°€ê²© ë°©í–¥
    df['price_direction_1d'] = (df['price_change_1d'] > 0).astype(int)
    df['price_direction_7d'] = (df['price_change_7d'] > 0).astype(int)
    
    return df


def calculate_correlations(
    whale_df: pd.DataFrame,
    target_df: pd.DataFrame,
    lags: list = [0, 1, 2, 3, 7]
) -> pd.DataFrame:
    """ìƒê´€ê´€ê³„ ê³„ì‚°"""
    # ë‚ ì§œ ê¸°ì¤€ ë³‘í•©
    merged = pd.merge(
        whale_df,
        target_df[['date', 'price_static', 'price_change_1d', 'price_change_7d', 
                   'price_change_30d', 'volatility_1d', 'volatility_7d',
                   'price_direction_1d', 'price_direction_7d']],
        on='date',
        how='inner'
    )
    
    results = []
    
    # ê³ ë˜/ì˜¨ì²´ì¸ ë³€ìˆ˜ í•„í„°ë§
    whale_cols = [
        'exchange_inflow_usd', 'exchange_outflow_usd', 'net_flow_usd',
        'active_addresses', 'large_tx_count', 'avg_tx_size_usd',
        'sum_open_interest', 'avg_funding_rate', 'long_short_ratio',
        'taker_buy_sell_ratio', 'top_trader_long_short_ratio',
        'top100_richest_pct', 'whale_conc_change_7d'
    ]
    
    # ì¡´ì¬í•˜ëŠ” ì»¬ëŸ¼ë§Œ ì‚¬ìš©
    available_whale_cols = [col for col in whale_cols if col in merged.columns]
    
    target_cols = ['price_static', 'price_change_1d', 'price_change_7d', 
                   'price_change_30d', 'volatility_1d', 'volatility_7d',
                   'price_direction_1d', 'price_direction_7d']
    
    for var in available_whale_cols:
        for target in target_cols:
            for lag in lags:
                if lag == 0:
                    x = merged[var]
                    y = merged[target]
                else:
                    x = merged[var].shift(lag)
                    y = merged[target]
                
                # ìœ íš¨í•œ ë°ì´í„°ë§Œ ì‚¬ìš©
                valid_mask = ~(x.isna() | y.isna())
                x_valid = x[valid_mask]
                y_valid = y[valid_mask]
                
                if len(x_valid) < 30:
                    continue
                
                # Pearson ìƒê´€ê³„ìˆ˜
                try:
                    pearson_r, pearson_p = stats.pearsonr(x_valid, y_valid)
                except:
                    pearson_r, pearson_p = np.nan, np.nan
                
                # Spearman ìƒê´€ê³„ìˆ˜
                try:
                    spearman_r, spearman_p = stats.spearmanr(x_valid, y_valid)
                except:
                    spearman_r, spearman_p = np.nan, np.nan
                
                results.append({
                    'variable': var,
                    'target': target,
                    'lag': lag,
                    'pearson_correlation': pearson_r,
                    'pearson_pvalue': pearson_p,
                    'spearman_correlation': spearman_r,
                    'spearman_pvalue': spearman_p,
                    'sample_size': len(x_valid)
                })
    
    return pd.DataFrame(results)


def granger_causality_test(
    x: pd.Series,
    y: pd.Series,
    maxlag: int = 3
) -> Dict:
    """ê·¸ë ˆì¸ì € ì¸ê³¼ê´€ê³„ ê²€ì •"""
    if not HAS_GRANGER:
        return {'pvalue': np.nan, 'significant': False}
    
    # ê²°ì¸¡ì¹˜ ì œê±°
    valid_mask = ~(x.isna() | y.isna())
    x_valid = x[valid_mask].values
    y_valid = y[valid_mask].values
    
    if len(x_valid) < 50:  # ìµœì†Œ ìƒ˜í”Œ ìˆ˜
        return {'pvalue': np.nan, 'significant': False}
    
    try:
        # ê·¸ë ˆì¸ì € ê²€ì • (xê°€ yì˜ ì›ì¸ì¸ì§€)
        test_result = grangercausalitytests(
            np.column_stack([y_valid, x_valid]),
            maxlag=maxlag,
            verbose=False
        )
        
        # ìµœì  lagì˜ p-value ì¶”ì¶œ
        pvalues = [test_result[i+1][0]['ssr_ftest'][1] for i in range(maxlag)]
        min_pvalue = min(pvalues)
        
        return {
            'pvalue': min_pvalue,
            'significant': min_pvalue < 0.05
        }
    except:
        return {'pvalue': np.nan, 'significant': False}


def linear_regression_analysis(
    x: pd.Series,
    y: pd.Series
) -> Dict:
    """ì„ í˜• íšŒê·€ ë¶„ì„"""
    valid_mask = ~(x.isna() | y.isna())
    x_valid = x[valid_mask].values.reshape(-1, 1)
    y_valid = y[valid_mask].values
    
    if len(x_valid) < 30:
        return {'coefficient': np.nan, 'r_squared': np.nan}
    
    try:
        from sklearn.linear_model import LinearRegression
        from sklearn.metrics import r2_score
        
        model = LinearRegression()
        model.fit(x_valid, y_valid)
        y_pred = model.predict(x_valid)
        
        return {
            'coefficient': model.coef_[0],
            'r_squared': r2_score(y_valid, y_pred)
        }
    except:
        return {'coefficient': np.nan, 'r_squared': np.nan}


def random_forest_importance(
    X: pd.DataFrame,
    y: pd.Series,
    top_n: int = 10
) -> pd.DataFrame:
    """ëœë¤ í¬ë ˆìŠ¤íŠ¸ í”¼ì²˜ ì¤‘ìš”ë„"""
    if not HAS_SKLEARN:
        return pd.DataFrame()
    
    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
    X_clean = X.fillna(0)
    y_clean = y.fillna(0)
    
    if len(X_clean) < 30:
        return pd.DataFrame()
    
    try:
        model = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)
        model.fit(X_clean, y_clean)
        
        importance_df = pd.DataFrame({
            'feature': X_clean.columns,
            'importance': model.feature_importances_
        }).sort_values('importance', ascending=False).head(top_n)
        
        return importance_df
    except:
        return pd.DataFrame()


def main():
    """ë©”ì¸ ì‹¤í–‰"""
    import argparse
    
    parser = argparse.ArgumentParser(description="ê³ ë˜ ë°ì´í„° ê°€ê²© ìƒê´€ê´€ê³„ ë¶„ì„")
    parser.add_argument("--coin", type=str, default="BTC", choices=["BTC", "ETH"])
    parser.add_argument("--start-date", type=str, default="2022-01-01")
    args = parser.parse_args()
    
    print("=" * 80)
    print("ğŸ“Š ê³ ë˜ ë°ì´í„° ê°€ê²© ìƒê´€ê´€ê³„ ë¶„ì„")
    print("=" * 80)
    print(f"ì½”ì¸: {args.coin}")
    print(f"ì‹œì‘ì¼: {args.start_date}")
    print()
    
    # 1. ê°€ê²© ë°ì´í„° ë¡œë“œ
    print("[1/5] ê°€ê²© ë°ì´í„° ë¡œë“œ ì¤‘...")
    price_df = load_price_data(args.coin, args.start_date)
    
    if price_df.empty:
        print("âŒ ê°€ê²© ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"   âœ… {len(price_df)}ê±´ ë¡œë“œ")
    
    # 2. ì¢…ì†ë³€ìˆ˜ ê³„ì‚°
    print("\n[2/5] ì¢…ì†ë³€ìˆ˜ ê³„ì‚° ì¤‘...")
    target_df = calculate_target_variables(price_df)
    print(f"   âœ… ì¢…ì†ë³€ìˆ˜ ê³„ì‚° ì™„ë£Œ")
    
    # 3. ê³ ë˜ ë°ì´í„° ë¡œë“œ
    print("\n[3/5] ê³ ë˜ ë°ì´í„° ë¡œë“œ ì¤‘...")
    fe = FeatureEngineer()
    raw_df = fe.load_raw_data(args.start_date, coin=args.coin)
    
    if raw_df.empty:
        print("âŒ ê³ ë˜ ë°ì´í„°ë¥¼ ë¶ˆëŸ¬ì˜¬ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"   âœ… {len(raw_df)}ê±´ ë¡œë“œ")
    
    # 4. ìƒê´€ê´€ê³„ ê³„ì‚°
    print("\n[4/5] ìƒê´€ê´€ê³„ ê³„ì‚° ì¤‘...")
    results_df = calculate_correlations(raw_df, target_df)
    
    if results_df.empty:
        print("âŒ ìƒê´€ê´€ê³„ë¥¼ ê³„ì‚°í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
        return
    
    print(f"   âœ… {len(results_df)}ê°œ ì¡°í•© ë¶„ì„")
    
    # 5. ì¶”ê°€ ë¶„ì„ (ê·¸ë ˆì¸ì €, íšŒê·€, RF)
    print("\n[5/5] ì¶”ê°€ ë¶„ì„ ì¤‘...")
    
    # ë‚ ì§œ ê¸°ì¤€ ë³‘í•©
    merged = pd.merge(raw_df, target_df, on='date', how='inner')
    
    # ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ë§Œ ì„ íƒ (p < 0.05)
    significant = results_df[results_df['pearson_pvalue'] < 0.05].copy()
    
    if len(significant) > 0:
        print(f"   âœ… ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„: {len(significant)}ê°œ")
        
        # ê·¸ë ˆì¸ì € ì¸ê³¼ê´€ê³„ ê²€ì • (ìƒ˜í”Œë§)
        if HAS_GRANGER:
            print("   ê·¸ë ˆì¸ì € ì¸ê³¼ê´€ê³„ ê²€ì • ì¤‘...")
            granger_results = []
            sample = significant.sample(min(20, len(significant)))  # ìµœëŒ€ 20ê°œ ìƒ˜í”Œ
            
            for _, row in sample.iterrows():
                var = row['variable']
                target = row['target']
                lag = int(row['lag'])
                
                if var in merged.columns and target in merged.columns:
                    x = merged[var].shift(lag) if lag > 0 else merged[var]
                    y = merged[target]
                    
                    granger = granger_causality_test(x, y)
                    granger_results.append({
                        'variable': var,
                        'target': target,
                        'lag': lag,
                        'granger_pvalue': granger['pvalue'],
                        'granger_significant': granger['significant']
                    })
            
            if granger_results:
                granger_df = pd.DataFrame(granger_results)
                significant = significant.merge(granger_df, on=['variable', 'target', 'lag'], how='left')
                print(f"      âœ… {len(granger_results)}ê°œ ê²€ì • ì™„ë£Œ")
        
        # ì„ í˜• íšŒê·€ ë¶„ì„
        print("   ì„ í˜• íšŒê·€ ë¶„ì„ ì¤‘...")
        regression_results = []
        
        for _, row in significant.head(50).iterrows():  # ìµœëŒ€ 50ê°œ
            var = row['variable']
            target = row['target']
            lag = int(row['lag'])
            
            if var in merged.columns and target in merged.columns:
                x = merged[var].shift(lag) if lag > 0 else merged[var]
                y = merged[target]
                
                reg = linear_regression_analysis(x, y)
                regression_results.append({
                    'variable': var,
                    'target': target,
                    'lag': lag,
                    'regression_coef': reg['coefficient'],
                    'r_squared': reg['r_squared']
                })
        
        if regression_results:
            reg_df = pd.DataFrame(regression_results)
            significant = significant.merge(reg_df, on=['variable', 'target', 'lag'], how='left')
            print(f"      âœ… {len(regression_results)}ê°œ íšŒê·€ ë¶„ì„ ì™„ë£Œ")
    
    # 6. ê²°ê³¼ ì €ì¥
    output_file = OUTPUT_DIR / f"whale_price_correlation_{args.coin}_{datetime.now().strftime('%Y%m%d_%H%M%S')}.csv"
    results_df.to_csv(output_file, index=False)
    print(f"\nâœ… ê²°ê³¼ ì €ì¥: {output_file}")
    
    # 7. ìš”ì•½ ì¶œë ¥
    print("\n" + "=" * 80)
    print("ğŸ“Š ìƒìœ„ 10ê°œ ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„")
    print("=" * 80)
    
    if len(significant) > 0:
        top10 = significant.nlargest(10, 'pearson_correlation', keep='all')
        display_cols = ['variable', 'target', 'lag', 'pearson_correlation', 'pearson_pvalue', 'sample_size']
        if 'r_squared' in top10.columns:
            display_cols.append('r_squared')
        print(top10[display_cols].to_string())
    else:
        print("ìœ ì˜ë¯¸í•œ ìƒê´€ê´€ê³„ê°€ ì—†ìŠµë‹ˆë‹¤.")
    
    print("\n" + "=" * 80)
    print("âœ… ë¶„ì„ ì™„ë£Œ")
    print("=" * 80)


if __name__ == "__main__":
    main()

