"""
ë°ì´í„° ë¡œë” ìœ í‹¸ë¦¬í‹°
- SQLiteì—ì„œ ê±°ë˜ì†Œ ë°ì´í„° ë¡œë“œ
- ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ë²”ìœ„ ì¡°íšŒ
- Streamlit Cloud í˜¸í™˜
"""

import sqlite3
import pandas as pd
from pathlib import Path
from datetime import datetime
from typing import Tuple, List, Optional
import os
import logging

# Streamlit ëª¨ë“ˆ ì¡°ê±´ë¶€ ì„í¬íŠ¸ ë° ë”ë¯¸ ìºì‹œ ë°ì½”ë ˆì´í„° ì •ì˜
try:
    import streamlit as st
except ImportError:
    st = None

def dummy_cache(ttl=None):
    def decorator(func):
        return func
    return decorator

# st.cache_dataê°€ ì—†ìœ¼ë©´ ë”ë¯¸ ë°ì½”ë ˆì´í„° ì‚¬ìš©
if st is None:
    st_cache_data = dummy_cache
else:
    st_cache_data = st.cache_data

# Streamlit Cloud ë˜ëŠ” ë¡œì»¬ í™˜ê²½ ê°ì§€
if os.path.exists('/mount/src'):
    # Streamlit Cloud
    ROOT = Path('/mount/src/whale-arbitrage')
    DB_PATH = Path('/tmp') / "project.db"
    USE_SUPABASE = True  # í´ë¼ìš°ë“œì—ì„œëŠ” Supabase ìš°ì„  ì‚¬ìš©
elif os.path.exists('/app'):
    # Docker ì»¨í…Œì´ë„ˆ ë‚´ë¶€
    ROOT = Path('/app')
    DB_PATH = ROOT / "data" / "project.db"
    USE_SUPABASE = False
else:
    # ë¡œì»¬ ê°œë°œ í™˜ê²½
    ROOT = Path(__file__).resolve().parents[2]
    DB_PATH = ROOT / "data" / "project.db"
    USE_SUPABASE = False


class DataLoader:
    def __init__(self):
        self.db_path = DB_PATH
        self.use_supabase = USE_SUPABASE
        self._supabase_client = None
        
        # Streamlit UIì— ë””ë²„ê·¸ ì •ë³´ í‘œì‹œ (Streamlit Cloudìš©)
        try:
            import streamlit as st
            with st.spinner("ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ì¤‘..."):
                self._initialize_database(st_module=st)
        except ImportError:
            # Streamlitì´ ì—†ëŠ” í™˜ê²½ (í…ŒìŠ¤íŠ¸ ë“±)
            self._initialize_database(st_module=None)
    
    def _get_supabase_client(self):
        """Supabase í´ë¼ì´ì–¸íŠ¸ ê°€ì ¸ì˜¤ê¸° (ì§€ì—° ì´ˆê¸°í™”)"""
        if self._supabase_client is None:
            try:
                from supabase import create_client
                from .secrets_helper import get_secret
                
                is_streamlit_cloud = os.path.exists('/mount/src')

                # Streamlit Cloudì—ì„œëŠ” "ì„œë¹„ìŠ¤ ë¡¤ í‚¤" ì‚¬ìš©ì„ í”¼í•˜ê³ , ì•±ì—ì„œëŠ” anon key ì‚¬ìš©ì„ ê¶Œì¥
                supabase_url = get_secret("SUPABASE_URL")
                supabase_key = get_secret("SUPABASE_KEY") or get_secret("SUPABASE_ANON_KEY")

                # ë¡œì»¬/ì„œë²„ ì‚¬ì´ë“œ ì‘ì—…(maintenance ìŠ¤í¬ë¦½íŠ¸ ë“±)ì—ì„œëŠ” ì„œë¹„ìŠ¤ ë¡¤ í‚¤ í´ë°± í—ˆìš©
                if not is_streamlit_cloud and not supabase_key:
                    supabase_key = get_secret("SUPABASE_SERVICE_ROLE_KEY")
                
                if not supabase_url or not supabase_key:
                    raise ValueError("Supabase í™˜ê²½ ë³€ìˆ˜ê°€ ì„¤ì •ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. SUPABASE_URLê³¼ SUPABASE_KEYë¥¼ í™•ì¸í•˜ì„¸ìš”.")
                
                self._supabase_client = create_client(supabase_url, supabase_key)
                logging.info("Supabase í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™” ì™„ë£Œ")
            except ImportError:
                logging.warning("supabase íŒ¨í‚¤ì§€ê°€ ì„¤ì¹˜ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤. SQLiteë§Œ ì‚¬ìš©í•©ë‹ˆë‹¤.")
                self.use_supabase = False
            except Exception as e:
                logging.warning(f"Supabase ì´ˆê¸°í™” ì‹¤íŒ¨: {e}. SQLiteë¡œ í´ë°±í•©ë‹ˆë‹¤.")
                self.use_supabase = False
        
        return self._supabase_client
    
    def _initialize_database(self, st_module=None):
        """ë°ì´í„°ë² ì´ìŠ¤ ì´ˆê¸°í™” ë¡œì§
        
        Args:
            st_module: streamlit ëª¨ë“ˆ (Streamlit ì‹¤í–‰ í™˜ê²½ì—ì„œë§Œ ì „ë‹¬). ì—†ìœ¼ë©´ UI ì¶œë ¥ ì—†ì´ ë™ì‘.
        """
        st = st_module
        
        # í™˜ê²½ ì •ë³´ (ë””ë²„ê¹…ìš©)
        is_streamlit_cloud = os.path.exists('/mount/src')
        debug_info = {
            "í™˜ê²½": "Streamlit Cloud" if is_streamlit_cloud else "ë¡œì»¬/Docker",
            "DB ê²½ë¡œ": str(self.db_path),
            "íŒŒì¼ ì¡´ì¬": self.db_path.exists(),
            "Supabase ì‚¬ìš©": self.use_supabase,
            "/tmp ì¡´ì¬": os.path.exists('/tmp'),
            "/mount/src ì¡´ì¬": os.path.exists('/mount/src'),
        }
        
        # Supabase ì‚¬ìš© ì‹œ SQLite íŒŒì¼ ì²´í¬ ê±´ë„ˆë›°ê¸°
        if self.use_supabase:
            try:
                supabase = self._get_supabase_client()
                if supabase:
                    if st:
                        st.success("âœ… Supabase ì—°ê²° ì„±ê³µ")
                    # Supabase ì‚¬ìš© ì‹œ SQLite íŒŒì¼ ë¶ˆí•„ìš”
                    self._conn = None
                    self._db_path = None
                    return
            except Exception as e:
                logging.warning(f"Supabase ì´ˆê¸°í™” ì‹¤íŒ¨, SQLiteë¡œ í´ë°±: {e}")
                self.use_supabase = False
        
        # ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì´ ì—†ìœ¼ë©´ ë‹¤ìš´ë¡œë“œ ì‹œë„ (Streamlit Cloudìš©)
        if not self.db_path.exists():
            try:
                if st:
                    st.info("ğŸ“¥ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ë‹¤ìš´ë¡œë“œí•˜ëŠ” ì¤‘...")
                self._download_database_if_needed()
                if self.db_path.exists():
                    if st:
                        st.success(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {self.db_path}")
                else:
                    if st:
                        st.error(f"âŒ ë‹¤ìš´ë¡œë“œ í›„ì—ë„ íŒŒì¼ì´ ì—†ìŠµë‹ˆë‹¤: {self.db_path}")
            except Exception as e:
                # ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨ ì‹œ ìƒì„¸í•œ ì—ëŸ¬ ë©”ì‹œì§€
                error_msg = f"ë°ì´í„°ë² ì´ìŠ¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}"
                logging.error(error_msg)
                try:
                    if st:
                        st.error(f"âŒ {error_msg}")
                        st.json(debug_info)
                except:
                    pass
                raise FileNotFoundError(
                    f"ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.db_path}\n"
                    f"ë‹¤ìš´ë¡œë“œ ì‹œë„ ì‹¤íŒ¨: {str(e)}\n"
                    f"Streamlit Cloudì˜ ê²½ìš° Secretsì— DATABASE_URLì´ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n"
                    f"ë””ë²„ê·¸ ì •ë³´: {debug_info}"
                ) from e
        
        if not self.db_path.exists():
            try:
                if st:
                    st.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.db_path}")
                    st.json(debug_info)
            except:
                pass
            raise FileNotFoundError(
                f"ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {self.db_path}\n"
                f"Streamlit Cloudì˜ ê²½ìš° Secretsì— DATABASE_URLì´ ì„¤ì •ë˜ì–´ ìˆëŠ”ì§€ í™•ì¸í•˜ì„¸ìš”.\n"
                f"ë””ë²„ê·¸ ì •ë³´: {debug_info}"
            )
        
        # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° (ì§€ì—° ì—°ê²° - í•„ìš”í•  ë•Œë§ˆë‹¤ ìƒˆë¡œ ì—°ê²°)
        # Streamlit Cloudì—ì„œëŠ” ë©€í‹°ìŠ¤ë ˆë“œ í™˜ê²½ì´ë¯€ë¡œ ì—°ê²°ì„ ìºì‹±í•˜ì§€ ì•ŠìŒ
        self._conn = None
        self._db_path = str(self.db_path)
        
        # ì´ˆê¸° ì—°ê²° í…ŒìŠ¤íŠ¸
        try:
            import streamlit as st
            with st.spinner("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í…ŒìŠ¤íŠ¸ ì¤‘..."):
                test_conn = sqlite3.connect(self._db_path, timeout=10.0, check_same_thread=False)
                cursor = test_conn.cursor()
                cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
                tables = cursor.fetchall()
                table_names = [t[0] for t in tables]
                test_conn.close()
                
                if len(tables) == 0:
                    error_msg = f"ë°ì´í„°ë² ì´ìŠ¤ì— í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œ: {self.db_path}"
                    logging.error(error_msg)
                    st.error(f"âŒ {error_msg}")
                    raise ValueError(error_msg)
                
                # í•„ìˆ˜ í…Œì´ë¸” í™•ì¸
                required_tables = ['upbit_daily', 'binance_spot_daily', 'bitget_spot_daily', 'exchange_rate']
                missing_tables = [t for t in required_tables if t not in table_names]
                if missing_tables:
                    warning_msg = f"ì¼ë¶€ í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤: {missing_tables}. ì¡´ì¬í•˜ëŠ” í…Œì´ë¸”: {table_names}"
                    logging.warning(warning_msg)
                    st.warning(f"âš ï¸ {warning_msg}")
                else:
                    st.success(f"âœ… ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì„±ê³µ ({len(tables)}ê°œ í…Œì´ë¸”)")
        except ImportError:
            # Streamlitì´ ì—†ëŠ” í™˜ê²½
            test_conn = sqlite3.connect(self._db_path, timeout=10.0, check_same_thread=False)
            cursor = test_conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            tables = cursor.fetchall()
            table_names = [t[0] for t in tables]
            test_conn.close()
            
            if len(tables) == 0:
                logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ì— í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œ: {self.db_path}")
                raise ValueError(f"ë°ì´í„°ë² ì´ìŠ¤ì— í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤. íŒŒì¼ ê²½ë¡œ: {self.db_path}")
            
            # í•„ìˆ˜ í…Œì´ë¸” í™•ì¸
            required_tables = ['upbit_daily', 'binance_spot_daily', 'bitget_spot_daily', 'exchange_rate']
            missing_tables = [t for t in required_tables if t not in table_names]
            if missing_tables:
                logging.warning(f"ì¼ë¶€ í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤: {missing_tables}. ì¡´ì¬í•˜ëŠ” í…Œì´ë¸”: {table_names}")
        except sqlite3.Error as e:
            error_msg = f"ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì‹¤íŒ¨: {str(e)}\níŒŒì¼ ê²½ë¡œ: {self.db_path}\níŒŒì¼ ì¡´ì¬: {self.db_path.exists()}"
            logging.error(error_msg)
            try:
                import streamlit as st
                st.error(f"âŒ {error_msg}")
            except:
                pass
            raise sqlite3.Error(error_msg) from e
    
    @property
    def conn(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° (í•„ìš”í•  ë•Œë§ˆë‹¤ ìƒˆë¡œ ìƒì„±)"""
        # Supabase ì‚¬ìš© ì‹œ SQLite ì—°ê²° ë¶ˆí•„ìš”
        if self.use_supabase:
            return None
        
        # _db_pathê°€ Noneì´ë©´ SQLite ì‚¬ìš© ì•ˆ í•¨
        if self._db_path is None:
            return None
        
        if self._conn is None:
            try:
                self._conn = sqlite3.connect(self._db_path, timeout=10.0, check_same_thread=False)
            except sqlite3.Error as e:
                logging.error(f"ë°ì´í„°ë² ì´ìŠ¤ ì¬ì—°ê²° ì‹¤íŒ¨: {str(e)}")
                raise
        return self._conn
    
    def _download_database_if_needed(self):
        """Streamlit Cloudì—ì„œ ë°ì´í„°ë² ì´ìŠ¤ ë‹¤ìš´ë¡œë“œ ë° ì••ì¶• í•´ì œ"""
        import streamlit as st
        
        try:
            # Secretsì—ì„œ DATABASE_URL ê°€ì ¸ì˜¤ê¸°
            db_url = None
            try:
                if hasattr(st, 'secrets'):
                    if hasattr(st.secrets, 'get'):
                        db_url = st.secrets.get("DATABASE_URL", None)
                    elif "DATABASE_URL" in st.secrets:
                        db_url = st.secrets["DATABASE_URL"]
            except (FileNotFoundError, AttributeError, KeyError, TypeError):
                # Streamlit secrets íŒŒì¼ì´ ì—†ê±°ë‚˜ í‚¤ê°€ ì—†ëŠ” ê²½ìš°
                pass
            except Exception as e:
                logging.warning(f"Secrets ì½ê¸° ì˜¤ë¥˜: {str(e)}")
            
            if not db_url:
                st.warning("âš ï¸ DATABASE_URLì´ Secretsì— ì„¤ì •ë˜ì–´ ìˆì§€ ì•ŠìŠµë‹ˆë‹¤.")
                return
            
            if not db_url:
                st.warning("âš ï¸ DATABASE_URLì´ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
                return
            
            st.info(f"ğŸ“¥ ë‹¤ìš´ë¡œë“œ URL: {db_url[:50]}...")
            
            import urllib.request
            import tarfile
            
            # ì„ì‹œ ë””ë ‰í† ë¦¬ ìƒì„±
            temp_dir = self.db_path.parent
            temp_dir.mkdir(parents=True, exist_ok=True)
            
            # URLì—ì„œ íŒŒì¼ í™•ì¥ì í™•ì¸
            if db_url.endswith('.tar.gz'):
                st.info("ğŸ“¦ .tar.gz íŒŒì¼ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                # .tar.gz íŒŒì¼ ë‹¤ìš´ë¡œë“œ
                temp_tar = temp_dir / "project.db.tar.gz"
                try:
                    urllib.request.urlretrieve(db_url, str(temp_tar))
                    st.info(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {temp_tar.stat().st_size / 1024:.2f} KB")
                except Exception as e:
                    st.error(f"âŒ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}")
                    raise
                
                # ì••ì¶• í•´ì œ
                st.info("ğŸ“‚ ì••ì¶• í•´ì œ ì¤‘...")
                try:
                    with tarfile.open(temp_tar, 'r:gz') as tar:
                        # ì••ì¶• í•´ì œ (temp_dirì—)
                        tar.extractall(temp_dir)
                    st.info("âœ… ì••ì¶• í•´ì œ ì™„ë£Œ")
                except Exception as e:
                    st.error(f"âŒ ì••ì¶• í•´ì œ ì‹¤íŒ¨: {str(e)}")
                    raise
                
                # ì„ì‹œ íŒŒì¼ ì‚­ì œ
                try:
                    temp_tar.unlink()
                except:
                    pass
                
                # ì••ì¶• í•´ì œëœ íŒŒì¼ í™•ì¸ (data/project.db í˜•íƒœë¡œ ì••ì¶•ë˜ì–´ ìˆìŒ)
                # 1ìˆœìœ„: data/project.db
                alt_path = temp_dir / "data" / "project.db"
                if alt_path.exists():
                    st.info(f"âœ… data/project.db ë°œê²¬: {alt_path}")
                    # ëª©ì ì§€ ë””ë ‰í† ë¦¬ ìƒì„±
                    self.db_path.parent.mkdir(parents=True, exist_ok=True)
                    # íŒŒì¼ ì´ë™
                    alt_path.rename(self.db_path)
                    st.success(f"âœ… íŒŒì¼ ì´ë™ ì™„ë£Œ: {self.db_path}")
                    # ë¹ˆ data ë””ë ‰í† ë¦¬ ì •ë¦¬ (ìˆëŠ” ê²½ìš°)
                    try:
                        if alt_path.parent.exists() and not any(alt_path.parent.iterdir()):
                            alt_path.parent.rmdir()
                    except:
                        pass
                # 2ìˆœìœ„: temp_dir/project.db
                elif (temp_dir / "project.db").exists():
                    extracted_db = temp_dir / "project.db"
                    st.info(f"âœ… project.db ë°œê²¬: {extracted_db}")
                    self.db_path.parent.mkdir(parents=True, exist_ok=True)
                    extracted_db.rename(self.db_path)
                    st.success(f"âœ… íŒŒì¼ ì´ë™ ì™„ë£Œ: {self.db_path}")
                else:
                    # ëª¨ë“  ê°€ëŠ¥í•œ ìœ„ì¹˜ í™•ì¸
                    all_db_files = list(temp_dir.rglob("*.db"))
                    all_files = list(temp_dir.rglob("*"))
                    error_msg = (
                        f"ì••ì¶• í•´ì œ í›„ ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.\n"
                        f"ì˜ˆìƒ ìœ„ì¹˜: {temp_dir / 'data' / 'project.db'} ë˜ëŠ” {temp_dir / 'project.db'}\n"
                        f"ë°œê²¬ëœ .db íŒŒì¼: {[str(f) for f in all_db_files]}\n"
                        f"ì••ì¶• í•´ì œëœ ëª¨ë“  íŒŒì¼: {[str(f.relative_to(temp_dir)) for f in all_files[:20]]}"
                    )
                    st.error(f"âŒ {error_msg}")
                    raise FileNotFoundError(error_msg)
            else:
                # .db íŒŒì¼ ì§ì ‘ ë‹¤ìš´ë¡œë“œ
                st.info("ğŸ“¥ .db íŒŒì¼ ì§ì ‘ ë‹¤ìš´ë¡œë“œ ì¤‘...")
                urllib.request.urlretrieve(db_url, str(self.db_path))
                st.success(f"âœ… ë‹¤ìš´ë¡œë“œ ì™„ë£Œ: {self.db_path}")
            
            # ë‹¤ìš´ë¡œë“œ ì„±ê³µ í™•ì¸
            if not self.db_path.exists():
                error_msg = f"ë‹¤ìš´ë¡œë“œ í›„ íŒŒì¼ì´ ì¡´ì¬í•˜ì§€ ì•ŠìŠµë‹ˆë‹¤: {self.db_path}"
                st.error(f"âŒ {error_msg}")
                raise FileNotFoundError(error_msg)
            
            # íŒŒì¼ í¬ê¸° í™•ì¸
            file_size = self.db_path.stat().st_size / 1024
            st.info(f"ğŸ“Š ë°ì´í„°ë² ì´ìŠ¤ íŒŒì¼ í¬ê¸°: {file_size:.2f} KB")
                
        except Exception as e:
            # êµ¬ì²´ì ì¸ ì—ëŸ¬ ë©”ì‹œì§€ (Streamlit Cloud ë¡œê·¸ì— í‘œì‹œ)
            error_msg = f"ë°ì´í„°ë² ì´ìŠ¤ ë‹¤ìš´ë¡œë“œ ì‹¤íŒ¨: {str(e)}\nê²½ë¡œ: {self.db_path}"
            # ë¡œê¹… (Streamlit Cloud ë¡œê·¸ì— í‘œì‹œë¨)
            import logging
            logging.error(error_msg)
            # Streamlitì´ ìˆëŠ” ê²½ìš° UIì—ë„ í‘œì‹œ
            try:
                import streamlit as st
                st.error(f"âŒ {error_msg}")
            except:
                pass
            raise FileNotFoundError(error_msg) from e
    
    def close(self):
        """ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° ì¢…ë£Œ"""
        if hasattr(self, '_conn') and self._conn is not None:
            try:
                self._conn.close()
            except:
                pass
            finally:
                self._conn = None
    
    def get_available_dates(self, coin: str = 'BTC') -> Tuple[Optional[str], Optional[str]]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ë²”ìœ„ (ìµœì†Œ, ìµœëŒ€) ë°˜í™˜"""
        if coin == 'BTC':
            market = 'KRW-BTC'
            symbol = 'BTCUSDT'
            coin_label = 'BTC'
        elif coin == 'ETH':
            market = 'KRW-ETH'
            symbol = 'ETHUSDT'
            coin_label = 'ETH'
        else:
            return None, None
        
        try:
            # Supabase ìš°ì„  ì‚¬ìš© (í´ë¼ìš°ë“œ í™˜ê²½)
            if self.use_supabase:
                try:
                    supabase = self._get_supabase_client()
                    if supabase:
                        # 1. RPC(get_common_date_range)ë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë²„ì‚¬ì´ë“œì—ì„œ ì •í™•í•œ ë²”ìœ„ ê³„ì‚° ì‹œë„
                        try:
                            rpc_resp = supabase.rpc('get_common_date_range', {
                                'p_market': market,
                                'p_symbol': symbol
                            }).execute()
                            
                            if rpc_resp.data and len(rpc_resp.data) > 0:
                                row = rpc_resp.data[0]
                                min_d, max_d = row.get('min_date'), row.get('max_date')
                                if min_d and max_d:
                                    return min_d, max_d
                        except Exception as rpc_e:
                            logging.warning(f"Supabase RPC í˜¸ì¶œ ì‹¤íŒ¨ (get_common_date_range): {rpc_e}. ê¸°ì¡´ ë°©ì‹ìœ¼ë¡œ í´ë°±í•©ë‹ˆë‹¤.")

                        # 2. RPC ì‹¤íŒ¨ ì‹œ ê¸°ì¡´ ë°©ì‹ (í´ë¼ì´ì–¸íŠ¸ ì‚¬ì´ë“œ êµì§‘í•© - row limit ì£¼ì˜)
                        # load_exchange_dataê°€ ì‚¬ìš©í•˜ëŠ” ì£¼ìš” í…Œì´ë¸”ë“¤ì˜ êµì§‘í•© ë‚ ì§œ ì¡°íšŒ
                        # 1. upbit_daily
                        upbit_dates = set()
                        upbit_resp = supabase.table("upbit_daily").select("date").eq("market", market).execute()
                        if upbit_resp.data:
                            upbit_dates = {row['date'] for row in upbit_resp.data}
                        
                        # 2. binance_spot_daily
                        binance_dates = set()
                        binance_resp = supabase.table("binance_spot_daily").select("date").eq("symbol", symbol).execute()
                        if binance_resp.data:
                            binance_dates = {row['date'] for row in binance_resp.data}
                            
                        # 3. exchange_rate (í™˜ìœ¨)
                        exchange_dates = set()
                        exchange_resp = supabase.table("exchange_rate").select("date").execute()
                        if exchange_resp.data:
                            exchange_dates = {row['date'] for row in exchange_resp.data}
                        
                        # êµì§‘í•© ê³„ì‚° (ìµœì†Œí•œ ì´ 3ê°œëŠ” ìˆì–´ì•¼ í•¨)
                        common_dates = upbit_dates & binance_dates & exchange_dates
                        
                        if common_dates:
                            dates = sorted(list(common_dates))
                            min_date = dates[0]
                            max_date = dates[-1]
                            return min_date, max_date
                        
                        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ None ë°˜í™˜
                        return None, None
                except Exception as e:
                    logging.warning(f"Supabaseì—ì„œ ë‚ ì§œ ì¡°íšŒ ì‹¤íŒ¨, SQLiteë¡œ í´ë°±: {e}")
                    # SQLiteë¡œ í´ë°±
            
            # SQLite ì‚¬ìš© (ë¡œì»¬ í™˜ê²½ ë˜ëŠ” Supabase ì‹¤íŒ¨ ì‹œ)
            # ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²° í™•ì¸
            if not hasattr(self, 'conn') or self.conn is None:
                import logging
                logging.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤")
                return None, None
            
            # ë¨¼ì € í…Œì´ë¸” ì¡´ì¬ í™•ì¸
            cursor = self.conn.cursor()
            cursor.execute("SELECT name FROM sqlite_master WHERE type='table'")
            existing_tables = {row[0] for row in cursor.fetchall()}
            
            required_tables = ['upbit_daily', 'binance_spot_daily', 'bitget_spot_daily', 'exchange_rate']
            missing_tables = [t for t in required_tables if t not in existing_tables]
            
            if missing_tables:
                import logging
                error_msg = f"í•„ìˆ˜ í…Œì´ë¸”ì´ ì—†ìŠµë‹ˆë‹¤: {missing_tables}. ì¡´ì¬í•˜ëŠ” í…Œì´ë¸”: {list(existing_tables)}"
                logging.error(error_msg)
                try:
                    import streamlit as st
                    st.error(f"âŒ í•„ìˆ˜ í…Œì´ë¸” ëˆ„ë½: {', '.join(missing_tables)}")
                except:
                    pass
                return None, None
            
            # SQL ì¿¼ë¦¬ ì‹¤í–‰ (pandas ëŒ€ì‹  ì§ì ‘ cursor ì‚¬ìš©)
            # load_exchange_dataì™€ ë™ì¼í•œ ë¡œì§ìœ¼ë¡œ êµì§‘í•© ì¡°íšŒ
            query = f"""
            SELECT 
                MIN(date) as min_date,
                MAX(date) as max_date
            FROM (
                SELECT date FROM upbit_daily WHERE market = '{market}'
                INTERSECT
                SELECT date FROM binance_spot_daily WHERE symbol = '{symbol}'
                INTERSECT
                SELECT date FROM exchange_rate
            )
            """
            
            # pandas.read_sql ëŒ€ì‹  ì§ì ‘ cursor ì‚¬ìš©
            cursor.execute(query)
            result = cursor.fetchone()
            
            if result and result[0] is not None:
                return result[0], result[1]
            return None, None
            
        except sqlite3.Error as e:
            import logging
            error_msg = f"SQL ì˜¤ë¥˜ (get_available_dates): {str(e)}\në°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ: {self.db_path}"
            logging.error(error_msg)
            try:
                import streamlit as st
                st.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(e)}")
            except:
                pass
            return None, None
        except Exception as e:
            import logging
            error_msg = f"get_available_dates ì˜¤ë¥˜: {str(e)}\në°ì´í„°ë² ì´ìŠ¤ ê²½ë¡œ: {self.db_path}"
            logging.error(error_msg)
            try:
                import streamlit as st
                st.error(f"âŒ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            except:
                pass
            return None, None
    
    def get_available_dates_list(self, coin: str = 'BTC', start_date: Optional[str] = None, end_date: Optional[str] = None) -> List[str]:
        """ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ëª©ë¡ ë°˜í™˜"""
        if coin == 'BTC':
            market = 'KRW-BTC'
            symbol = 'BTCUSDT'
            coin_label = 'BTC'
        elif coin == 'ETH':
            market = 'KRW-ETH'
            symbol = 'ETHUSDT'
            coin_label = 'ETH'
        else:
            return []
        
        try:
            # Supabase ìš°ì„  ì‚¬ìš© (í´ë¼ìš°ë“œ í™˜ê²½)
            if self.use_supabase:
                try:
                    supabase = self._get_supabase_client()
                    if supabase:
                        # binance_futures_metricsì—ì„œ ë‚ ì§œ ëª©ë¡ ì¡°íšŒ
                        query = supabase.table("binance_futures_metrics") \
                            .select("date") \
                            .eq("symbol", symbol)
                        
                        if start_date:
                            query = query.gte("date", start_date)
                        if end_date:
                            query = query.lte("date", end_date)
                        
                        response = query.order("date").execute()
                        
                        if response.data and len(response.data) > 0:
                            dates = sorted(list(set([row['date'] for row in response.data])))
                            return dates
                        
                        # ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ ë¦¬ìŠ¤íŠ¸ ë°˜í™˜
                        return []
                except Exception as e:
                    logging.warning(f"Supabaseì—ì„œ ë‚ ì§œ ëª©ë¡ ì¡°íšŒ ì‹¤íŒ¨, SQLiteë¡œ í´ë°±: {e}")
                    # SQLiteë¡œ í´ë°±
            
            # SQLite ì‚¬ìš© (ë¡œì»¬ í™˜ê²½ ë˜ëŠ” Supabase ì‹¤íŒ¨ ì‹œ)
            if not hasattr(self, 'conn') or self.conn is None:
                logging.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤")
                return []
            
            date_filter = ""
            if start_date and end_date:
                date_filter = f"AND date BETWEEN '{start_date}' AND '{end_date}'"
            
            query = f"""
            SELECT DISTINCT date
            FROM (
                SELECT date FROM upbit_daily WHERE market = '{market}' {date_filter}
                INTERSECT
                SELECT date FROM binance_spot_daily WHERE symbol = '{symbol}' {date_filter}
                INTERSECT
                SELECT date FROM bitget_spot_daily WHERE symbol = '{symbol}' {date_filter}
                INTERSECT
                SELECT date FROM exchange_rate WHERE 1=1 {date_filter} -- í™˜ìœ¨ ë°ì´í„°ë„ í•„ìˆ˜
            )
            ORDER BY date
            """
            # pandas.read_sql ëŒ€ì‹  ì§ì ‘ cursor ì‚¬ìš©
            cursor = self.conn.cursor()
            cursor.execute(query)
            results = cursor.fetchall()
            return [row[0] for row in results]
        except sqlite3.Error as e:
            error_msg = f"SQL ì˜¤ë¥˜ (get_available_dates_list): {str(e)}"
            logging.error(error_msg)
            try:
                import streamlit as st
                st.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(e)}")
            except:
                pass
            return []
        except Exception as e:
            error_msg = f"get_available_dates_list ì˜¤ë¥˜: {str(e)}"
            logging.error(error_msg)
            try:
                import streamlit as st
                st.error(f"âŒ ë°ì´í„° ì¡°íšŒ ì˜¤ë¥˜: {str(e)}")
            except:
                pass
            return []

    def check_date_available(self, target_date: str, coin: str = 'BTC') -> Tuple[bool, Optional[str], Optional[int]]:
        """íŠ¹ì • ë‚ ì§œì˜ ë°ì´í„° ì¡´ì¬ ì—¬ë¶€ í™•ì¸ ë° ê°€ì¥ ê°€ê¹Œìš´ ë‚ ì§œ ë°˜í™˜"""
        available_dates = self.get_available_dates_list(coin)
        target_dt = datetime.strptime(target_date, "%Y-%m-%d").date()
        
        if target_date in available_dates:
            return True, target_date, 0
        
        # ê°€ì¥ ê°€ê¹Œìš´ ë‚ ì§œ ì°¾ê¸°
        if not available_dates:
            return False, None, None
        
        available_dts = [datetime.strptime(d, "%Y-%m-%d").date() for d in available_dates]
        
        closest_dt = min(available_dts, key=lambda d: abs((d - target_dt).days))
        days_diff = abs((closest_dt - target_dt).days)
        
        return False, closest_dt.strftime("%Y-%m-%d"), days_diff

    @st_cache_data(ttl=3600)
    def load_exchange_data(
        self, 
        start_date: str, 
        end_date: str, 
        coin: str = 'BTC'
    ) -> pd.DataFrame:
        """ê±°ë˜ì†Œ ë°ì´í„° ë¡œë“œ ë° ë³‘í•©"""
        if coin == 'BTC':
            market = 'KRW-BTC'
            symbol = 'BTCUSDT'
        elif coin == 'ETH':
            market = 'KRW-ETH'
            symbol = 'ETHUSDT'
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì½”ì¸: {coin}")
        
        # Supabase ìš°ì„  ì‚¬ìš© (í´ë¼ìš°ë“œ í™˜ê²½)
        if self.use_supabase:
            try:
                supabase = self._get_supabase_client()
                if supabase:
                    # ê° í…Œì´ë¸”ì—ì„œ ë°ì´í„° ë¡œë“œ
                    # 1. upbit_daily
                    upbit_response = supabase.table("upbit_daily") \
                        .select("date, trade_price") \
                        .eq("market", market) \
                        .gte("date", start_date) \
                        .lte("date", end_date) \
                        .order("date") \
                        .execute()
                    
                    if not upbit_response.data or len(upbit_response.data) == 0:
                        return pd.DataFrame()
                    
                    df = pd.DataFrame(upbit_response.data)
                    df['date'] = pd.to_datetime(df['date'])
                    df = df.rename(columns={'trade_price': 'upbit_price'})
                    
                    # 2. binance_spot_daily
                    binance_response = supabase.table("binance_spot_daily") \
                        .select("date, close") \
                        .eq("symbol", symbol) \
                        .gte("date", start_date) \
                        .lte("date", end_date) \
                        .order("date") \
                        .execute()
                    
                    if binance_response.data and len(binance_response.data) > 0:
                        binance_df = pd.DataFrame(binance_response.data)
                        binance_df['date'] = pd.to_datetime(binance_df['date'])
                        df = pd.merge(df, binance_df[['date', 'close']], on='date', how='left')
                        df = df.rename(columns={'close': 'binance_price'})
                    else:
                        df['binance_price'] = None
                    
                    # 3. bitget_spot_daily
                    bitget_response = supabase.table("bitget_spot_daily") \
                        .select("date, close") \
                        .eq("symbol", symbol) \
                        .gte("date", start_date) \
                        .lte("date", end_date) \
                        .order("date") \
                        .execute()
                    
                    if bitget_response.data and len(bitget_response.data) > 0:
                        bitget_df = pd.DataFrame(bitget_response.data)
                        bitget_df['date'] = pd.to_datetime(bitget_df['date'])
                        df = pd.merge(df, bitget_df[['date', 'close']], on='date', how='left')
                        df = df.rename(columns={'close': 'bitget_price'})
                    else:
                        df['bitget_price'] = None
                    
                    # 4. bybit_spot_daily
                    bybit_response = supabase.table("bybit_spot_daily") \
                        .select("date, close") \
                        .eq("symbol", symbol) \
                        .gte("date", start_date) \
                        .lte("date", end_date) \
                        .order("date") \
                        .execute()
                    
                    if bybit_response.data and len(bybit_response.data) > 0:
                        bybit_df = pd.DataFrame(bybit_response.data)
                        bybit_df['date'] = pd.to_datetime(bybit_df['date'])
                        df = pd.merge(df, bybit_df[['date', 'close']], on='date', how='left')
                        df = df.rename(columns={'close': 'bybit_price'})
                    else:
                        df['bybit_price'] = None
                    
                    # 5. exchange_rate
                    exchange_response = supabase.table("exchange_rate") \
                        .select("date, krw_usd") \
                        .gte("date", start_date) \
                        .lte("date", end_date) \
                        .order("date") \
                        .execute()
                    
                    if exchange_response.data and len(exchange_response.data) > 0:
                        exchange_df = pd.DataFrame(exchange_response.data)
                        exchange_df['date'] = pd.to_datetime(exchange_df['date'])
                        df = pd.merge(df, exchange_df[['date', 'krw_usd']], on='date', how='left')
                    else:
                        df['krw_usd'] = None
                    
                    # í™˜ìœ¨ ê²°ì¸¡ì¹˜ ì²˜ë¦¬
                    df['krw_usd'] = df['krw_usd'].ffill().bfill()
                    if df['krw_usd'].isna().any():
                        df['krw_usd'] = df['krw_usd'].interpolate(method='linear', limit_direction='both')
                    if df['krw_usd'].isna().any():
                        mean_rate = df['krw_usd'].mean()
                        if pd.notna(mean_rate):
                            df['krw_usd'] = df['krw_usd'].fillna(mean_rate)
                        else:
                            df['krw_usd'] = 0.0
                    
                    # USDT ê°€ê²©ì„ ì›í™”ë¡œ í™˜ì‚°
                    df['binance_krw'] = df['binance_price'] * df['krw_usd']
                    df['bitget_krw'] = df['bitget_price'] * df['krw_usd']
                    df['bybit_krw'] = df['bybit_price'] * df['krw_usd']
                    
                    return df
            except Exception as e:
                logging.warning(f"Supabaseì—ì„œ ê±°ë˜ì†Œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨, SQLiteë¡œ í´ë°±: {e}")
                # SQLiteë¡œ í´ë°±
        
        # SQLite ì‚¬ìš© (ë¡œì»¬ í™˜ê²½ ë˜ëŠ” Supabase ì‹¤íŒ¨ ì‹œ)
        try:
            if not hasattr(self, 'conn') or self.conn is None:
                logging.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤")
                return pd.DataFrame()
            
            query = f"""
            SELECT 
                u.date,
                u.trade_price as upbit_price,
                b.close as binance_price,
                bg.close as bitget_price,
                bb.close as bybit_price,
                e.krw_usd
            FROM upbit_daily u
            LEFT JOIN binance_spot_daily b ON u.date = b.date AND b.symbol = '{symbol}'
            LEFT JOIN bitget_spot_daily bg ON u.date = bg.date AND bg.symbol = '{symbol}'
            LEFT JOIN bybit_spot_daily bb ON u.date = bb.date AND bb.symbol = '{symbol}'
            LEFT JOIN exchange_rate e ON u.date = e.date
            WHERE u.market = '{market}'
            AND u.date BETWEEN '{start_date}' AND '{end_date}'
            ORDER BY u.date
            """
            
            # pandas.read_sql ì‚¬ìš© (JOIN ì¿¼ë¦¬ëŠ” ë³µì¡í•˜ë¯€ë¡œ pandas ì‚¬ìš©)
            df = pd.read_sql(query, self.conn)
        except sqlite3.Error as e:
            error_msg = f"SQL ì˜¤ë¥˜ (load_exchange_data): {str(e)}"
            logging.error(error_msg)
            try:
                import streamlit as st
                st.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(e)}")
            except:
                pass
            return pd.DataFrame()
        except Exception as e:
            error_msg = f"load_exchange_data ì˜¤ë¥˜: {str(e)}"
            logging.error(error_msg)
            try:
                import streamlit as st
                st.error(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            except:
                pass
            return pd.DataFrame()
        if len(df) == 0:
            return df
        
        df['date'] = pd.to_datetime(df['date'])
        
        # í™˜ìœ¨ ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (ì£¼ë§/ê³µíœ´ì¼ ëŒ€ì‘)
        # ë°ì´í„°ë² ì´ìŠ¤ì—ì„œ ì´ë¯¸ ë³´ì™„ë˜ì—ˆì§€ë§Œ, í˜¹ì‹œ ëª¨ë¥¼ ê²½ìš°ë¥¼ ëŒ€ë¹„í•œ ì¶”ê°€ ì²˜ë¦¬
        # 1. ì•ì˜ ê°’ìœ¼ë¡œ ì±„ìš°ê¸° (forward fill)
        df['krw_usd'] = df['krw_usd'].ffill()
        # 2. ë’¤ì˜ ê°’ìœ¼ë¡œ ì±„ìš°ê¸° (backward fill) - ì²˜ìŒì— NULLì´ ìˆëŠ” ê²½ìš°
        df['krw_usd'] = df['krw_usd'].bfill()
        # 3. ê·¸ë˜ë„ NULLì´ ìˆìœ¼ë©´ ì„ í˜• ë³´ê°„
        if df['krw_usd'].isna().any():
            df['krw_usd'] = df['krw_usd'].interpolate(method='linear', limit_direction='both')
        # 4. ìµœí›„ì˜ ìˆ˜ë‹¨: í‰ê· ê°’ìœ¼ë¡œ ì±„ìš°ê¸°
        if df['krw_usd'].isna().any():
            mean_rate = df['krw_usd'].mean()
            if pd.notna(mean_rate):
                df['krw_usd'] = df['krw_usd'].fillna(mean_rate)
            else: # ëª¨ë“  ê°’ì´ NaNì¸ ê²½ìš° (ë°ì´í„°ê°€ ê·¹íˆ ì ê±°ë‚˜ ì—†ìŒ)
                df['krw_usd'] = 0.0 # ë˜ëŠ” ì ì ˆí•œ ê¸°ë³¸ê°’
        
        # USDT ê°€ê²©ì„ ì›í™”ë¡œ í™˜ì‚°
        df['binance_krw'] = df['binance_price'] * df['krw_usd']
        df['bitget_krw'] = df['bitget_price'] * df['krw_usd']
        df['bybit_krw'] = df['bybit_price'] * df['krw_usd']
        
        return df
    
    def validate_date_range(self, start_date: str, end_date: str, coin: str = 'BTC') -> Tuple[bool, str]:
        """ë‚ ì§œ ë²”ìœ„ ê²€ì¦"""
        # ë‚ ì§œ í˜•ì‹ ê²€ì¦
        try:
            start_dt = datetime.strptime(start_date, "%Y-%m-%d").date()
            end_dt = datetime.strptime(end_date, "%Y-%m-%d").date()
        except ValueError:
            return False, "ë‚ ì§œ í˜•ì‹ì´ ì˜¬ë°”ë¥´ì§€ ì•ŠìŠµë‹ˆë‹¤. (YYYY-MM-DD í˜•ì‹ í•„ìš”)"
        
        # ì‹œì‘ ë‚ ì§œê°€ ì¢…ë£Œ ë‚ ì§œë³´ë‹¤ ëŠ¦ì€ ê²½ìš°
        if start_dt > end_dt:
            return False, "ì‹œì‘ ë‚ ì§œê°€ ì¢…ë£Œ ë‚ ì§œë³´ë‹¤ ëŠ¦ìŠµë‹ˆë‹¤."
        
        # ìµœì†Œ 30ì¼ ì´ìƒì˜ ë°ì´í„° í•„ìš”
        days_diff = (end_dt - start_dt).days
        if days_diff < 30:
            return False, f"ìµœì†Œ 30ì¼ ì´ìƒì˜ ê¸°ê°„ì´ í•„ìš”í•©ë‹ˆë‹¤. (í˜„ì¬: {days_diff}ì¼)"
        
        # ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ë²”ìœ„ í™•ì¸
        min_date, max_date = self.get_available_dates(coin)
        if not min_date or not max_date:
            return False, f"{coin}ì— ëŒ€í•œ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤."
        
        min_dt = datetime.strptime(min_date, "%Y-%m-%d").date()
        max_dt = datetime.strptime(max_date, "%Y-%m-%d").date()
        
        # ìš”ì²­í•œ ë‚ ì§œ ë²”ìœ„ê°€ ì‚¬ìš© ê°€ëŠ¥í•œ ë²”ìœ„ë¥¼ ë²—ì–´ë‚˜ëŠ” ê²½ìš°
        if start_dt < min_dt or end_dt > max_dt:
            return False, f"ì‚¬ìš© ê°€ëŠ¥í•œ ë‚ ì§œ ë²”ìœ„ëŠ” {min_date} ~ {max_date}ì…ë‹ˆë‹¤."
        
        # ì‹¤ì œ ë°ì´í„° ì¡´ì¬ í™•ì¸
        available_dates = self.get_available_dates_list(coin, start_date, end_date)
        if len(available_dates) < 30:
            return False, f"ì„ íƒí•œ ê¸°ê°„ì— ì‚¬ìš© ê°€ëŠ¥í•œ ë°ì´í„°ê°€ ë¶€ì¡±í•©ë‹ˆë‹¤. (í•„ìš”: 30ì¼ ì´ìƒ, í˜„ì¬: {len(available_dates)}ì¼)"
        
        return True, ""
    
    @st_cache_data(ttl=3600)
    def load_risk_data(self, start_date: str, end_date: str, coin: str = 'BTC') -> pd.DataFrame:
        """Project 3 (Risk AI) ë°ì´í„° ë¡œë“œ
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
            end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
            coin: ì½”ì¸ ì‹¬ë³¼ ('BTC' ë˜ëŠ” 'ETH', ê¸°ë³¸ê°’: 'BTC')
        
        Returns:
            DataFrame with columns:
            - date: ë‚ ì§œ
            - symbol: ì‹¬ë³¼ (ì˜ˆ: 'BTCUSDT')
            - avg_funding_rate: í‰ê·  í€ë”©ë¹„
            - sum_open_interest: ë¯¸ê²°ì œì•½ì • í•©ê³„
            - long_short_ratio: ë¡±/ìˆ ë¹„ìœ¨
            - volatility_24h: 24ì‹œê°„ ë³€ë™ì„±
            - top100_richest_pct: Top 100 ì§€ê°‘ ë³´ìœ  ë¹„ì¤‘
            - avg_transaction_value_btc: í‰ê·  ê±°ë˜ ê¸ˆì•¡ (BTC)
        """
        if coin == 'BTC':
            symbol = 'BTCUSDT'
            coin_label = 'BTC'
        elif coin == 'ETH':
            symbol = 'ETHUSDT'
            coin_label = 'ETH'
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì½”ì¸: {coin}")
        
        # Supabase ìš°ì„  ì‚¬ìš© (í´ë¼ìš°ë“œ í™˜ê²½)
        if self.use_supabase:
            try:
                supabase = self._get_supabase_client()
                if supabase:
                    # binance_futures_metrics ë¡œë“œ
                    futures_response = supabase.table("binance_futures_metrics") \
                        .select("*") \
                        .eq("symbol", symbol) \
                        .gte("date", start_date) \
                        .lte("date", end_date) \
                        .order("date") \
                        .execute()
                    
                    if futures_response.data and len(futures_response.data) > 0:
                        df = pd.DataFrame(futures_response.data)
                        df['date'] = pd.to_datetime(df['date'])
                        
                        # bitinfocharts_whale ë°ì´í„° ë³‘í•©
                        whale_response = supabase.table("bitinfocharts_whale") \
                            .select("*") \
                            .eq("coin", coin_label) \
                            .gte("date", start_date) \
                            .lte("date", end_date) \
                            .order("date") \
                            .execute()
                        
                        if whale_response.data and len(whale_response.data) > 0:
                            whale_df = pd.DataFrame(whale_response.data)
                            whale_df['date'] = pd.to_datetime(whale_df['date'])
                            df = pd.merge(df, whale_df[['date', 'top100_richest_pct', 'avg_transaction_value_btc']], 
                                        on='date', how='left')
                        
                        # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                        numeric_columns = [
                            'avg_funding_rate', 'sum_open_interest', 'long_short_ratio',
                            'volatility_24h', 'top100_richest_pct', 'avg_transaction_value_btc'
                        ]
                        for col in numeric_columns:
                            if col in df.columns:
                                df[col] = pd.to_numeric(df[col], errors='coerce')
                        
                        # whale ë°ì´í„°ëŠ” ì„ íƒì ì´ë¯€ë¡œ, íŒŒìƒìƒí’ˆ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìœ ì§€
                        # whale ì»¬ëŸ¼ë§Œ forward fillí•˜ê³ , íŒŒìƒìƒí’ˆ í•µì‹¬ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ í–‰ ìœ ì§€
                        whale_cols = ['top100_richest_pct', 'avg_transaction_value_btc']
                        core_cols = ['avg_funding_rate', 'sum_open_interest', 'volatility_24h']
                        
                        # whale ì»¬ëŸ¼ë§Œ forward fill í›„ 0ìœ¼ë¡œ ì±„ìš°ê¸° (NaN ë°©ì§€)
                        for col in whale_cols:
                            if col in df.columns:
                                df[col] = df[col].ffill().fillna(0.0)
                        
                        # í•µì‹¬ íŒŒìƒìƒí’ˆ ì»¬ëŸ¼ ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ í–‰ ìœ ì§€
                        # (whale ë°ì´í„°ê°€ ì—†ì–´ë„ íŒŒìƒìƒí’ˆ ë°ì´í„°ëŠ” ë°˜í™˜)
                        if len(core_cols) > 0:
                            has_core_data = df[core_cols].notna().any(axis=1)
                            df = df[has_core_data]
                        
                        # ì „ì²´ ë°ì´í„°í”„ë ˆì„ì— ëŒ€í•´ ìµœì¢…ì ìœ¼ë¡œ NaNì„ 0ìœ¼ë¡œ ì±„ìš°ê¸° (ì•ˆì „ì¥ì¹˜)
                        df = df.fillna(0.0)
                        
                        return df
                    else:
                        # Supabaseì—ì„œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ DataFrame ë°˜í™˜
                        logging.warning(f"Supabaseì—ì„œ {symbol} ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ê¸°ê°„: {start_date} ~ {end_date})")
                        return pd.DataFrame()
            except Exception as e:
                logging.warning(f"Supabaseì—ì„œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨, SQLiteë¡œ í´ë°±: {e}")
                # SQLiteë¡œ í´ë°±
        
        # SQLite ì‚¬ìš© (ë¡œì»¬ í™˜ê²½ ë˜ëŠ” Supabase ì‹¤íŒ¨ ì‹œ)
        try:
            if not hasattr(self, 'conn') or self.conn is None:
                logging.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤")
                return pd.DataFrame()
            
            query = f"""
            SELECT 
                f.date,
                f.symbol,
                f.avg_funding_rate,
                f.sum_open_interest,
                f.long_short_ratio,
                f.volatility_24h,
                b.top100_richest_pct,
                b.avg_transaction_value_btc
            FROM binance_futures_metrics f
            LEFT JOIN bitinfocharts_whale b ON f.date = b.date AND b.coin = '{coin_label}'
            WHERE f.symbol = '{symbol}'
            AND f.date BETWEEN '{start_date}' AND '{end_date}'
            ORDER BY f.date
            """
            
            df = pd.read_sql(query, self.conn)
            
            if len(df) == 0:
                return df
            
            df['date'] = pd.to_datetime(df['date'])
            
            # ìˆ«ì ì»¬ëŸ¼ì„ ëª…ì‹œì ìœ¼ë¡œ floatë¡œ ë³€í™˜ (SQLiteì—ì„œ objectë¡œ ì½íˆëŠ” ê²½ìš° ë°©ì§€)
            numeric_columns = [
                'avg_funding_rate',
                'sum_open_interest',
                'long_short_ratio',
                'volatility_24h',
                'top100_richest_pct',
                'avg_transaction_value_btc'
            ]
            
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬ (Forward Fill)
            # whale ë°ì´í„°ëŠ” ì„ íƒì ì´ë¯€ë¡œ, íŒŒìƒìƒí’ˆ ë°ì´í„°ê°€ ìˆìœ¼ë©´ ìœ ì§€
            # whale ì»¬ëŸ¼ë§Œ forward fillí•˜ê³ , íŒŒìƒìƒí’ˆ í•µì‹¬ ì»¬ëŸ¼ì´ ìˆìœ¼ë©´ í–‰ ìœ ì§€
            whale_cols = ['top100_richest_pct', 'avg_transaction_value_btc']
            core_cols = ['avg_funding_rate', 'sum_open_interest', 'volatility_24h']
            
            # whale ì»¬ëŸ¼ë§Œ forward fill
            for col in whale_cols:
                if col in df.columns:
                    df[col] = df[col].ffill()
            
            # í•µì‹¬ íŒŒìƒìƒí’ˆ ì»¬ëŸ¼ ì¤‘ í•˜ë‚˜ë¼ë„ ìˆìœ¼ë©´ í–‰ ìœ ì§€
            # (whale ë°ì´í„°ê°€ ì—†ì–´ë„ íŒŒìƒìƒí’ˆ ë°ì´í„°ëŠ” ë°˜í™˜)
            if len(core_cols) > 0:
                has_core_data = df[core_cols].notna().any(axis=1)
                df = df[has_core_data]
            
            return df
            
        except sqlite3.Error as e:
            error_msg = f"SQL ì˜¤ë¥˜ (load_risk_data): {str(e)}"
            logging.error(error_msg)
            try:
                import streamlit as st
                st.error(f"âŒ ë°ì´í„°ë² ì´ìŠ¤ ì˜¤ë¥˜: {str(e)}")
            except:
                pass
            return pd.DataFrame()
        except Exception as e:
            error_msg = f"load_risk_data ì˜¤ë¥˜: {str(e)}"
            logging.error(error_msg)
            try:
                import streamlit as st
                st.error(f"âŒ ë°ì´í„° ë¡œë“œ ì˜¤ë¥˜: {str(e)}")
            except:
                pass
            return pd.DataFrame()
    
    @st_cache_data(ttl=3600)
    def load_futures_extended_metrics(self, start_date: str, end_date: str, symbol: str = 'BTCUSDT') -> pd.DataFrame:
        """íŒŒìƒìƒí’ˆ í™•ì¥ ì§€í‘œ ë¡œë“œ (futures_extended_metrics)
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
            end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
            symbol: ì‹¬ë³¼ (ê¸°ë³¸ê°’: 'BTCUSDT')
        
        Returns:
            DataFrame with futures extended metrics
        """
        # Supabase ìš°ì„  ì‚¬ìš© (í´ë¼ìš°ë“œ í™˜ê²½)
        if self.use_supabase:
            try:
                supabase = self._get_supabase_client()
                if supabase:
                    response = supabase.table("futures_extended_metrics") \
                        .select("*") \
                        .eq("symbol", symbol) \
                        .gte("date", start_date) \
                        .lte("date", end_date) \
                        .order("date") \
                        .execute()
                    
                    if response.data and len(response.data) > 0:
                        df = pd.DataFrame(response.data)
                        df['date'] = pd.to_datetime(df['date'])
                        
                        # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                        numeric_columns = [
                            'long_short_ratio', 'long_account_pct', 'short_account_pct',
                            'taker_buy_sell_ratio', 'taker_buy_vol', 'taker_sell_vol',
                            'top_trader_long_short_ratio', 'bybit_funding_rate', 'bybit_oi'
                        ]
                        for col in numeric_columns:
                            if col in df.columns:
                                df[col] = pd.to_numeric(df[col], errors='coerce')
                        
                        return df
                    else:
                        # Supabaseì—ì„œ ë°ì´í„°ê°€ ì—†ìœ¼ë©´ ë¹ˆ DataFrame ë°˜í™˜
                        logging.warning(f"Supabaseì—ì„œ {symbol} futures_extended_metrics ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤ (ê¸°ê°„: {start_date} ~ {end_date})")
                        return pd.DataFrame()
            except Exception as e:
                logging.warning(f"Supabaseì—ì„œ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨, SQLiteë¡œ í´ë°±: {e}")
                # SQLiteë¡œ í´ë°±
        
        # SQLite ì‚¬ìš© (ë¡œì»¬ í™˜ê²½ ë˜ëŠ” Supabase ì‹¤íŒ¨ ì‹œ)
        try:
            if not hasattr(self, 'conn') or self.conn is None:
                logging.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤")
                return pd.DataFrame()
            
            query = f"""
            SELECT 
                date,
                symbol,
                long_short_ratio,
                long_account_pct,
                short_account_pct,
                taker_buy_sell_ratio,
                taker_buy_vol,
                taker_sell_vol,
                top_trader_long_short_ratio,
                bybit_funding_rate,
                bybit_oi
            FROM futures_extended_metrics
            WHERE symbol = '{symbol}'
            AND date BETWEEN '{start_date}' AND '{end_date}'
            ORDER BY date
            """
            
            df = pd.read_sql(query, self.conn)
            
            if len(df) == 0:
                return df
            
            df['date'] = pd.to_datetime(df['date'])
            
            # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
            numeric_columns = [
                'long_short_ratio',
                'long_account_pct',
                'short_account_pct',
                'taker_buy_sell_ratio',
                'taker_buy_vol',
                'taker_sell_vol',
                'top_trader_long_short_ratio',
                'bybit_funding_rate',
                'bybit_oi'
            ]
            
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            return df
            
        except sqlite3.Error as e:
            error_msg = f"SQL ì˜¤ë¥˜ (load_futures_extended_metrics): {str(e)}"
            logging.error(error_msg)
            return pd.DataFrame()
        except Exception as e:
            error_msg = f"load_futures_extended_metrics ì˜¤ë¥˜: {str(e)}"
            logging.error(error_msg)
            return pd.DataFrame()
    
    @st_cache_data(ttl=3600)
    def load_risk_data_weekly(self, start_date: str, end_date: str, coin: str = 'BTC') -> pd.DataFrame:
        """Project 3 (Risk AI) ì£¼ë´‰ ë°ì´í„° ë¡œë“œ
        
        Args:
            start_date: ì‹œì‘ ë‚ ì§œ (YYYY-MM-DD)
            end_date: ì¢…ë£Œ ë‚ ì§œ (YYYY-MM-DD)
            coin: ì½”ì¸ ì‹¬ë³¼ ('BTC' ë˜ëŠ” 'ETH', ê¸°ë³¸ê°’: 'BTC')
        
        Returns:
            DataFrame with weekly aggregated data
        """
        if coin == 'BTC':
            symbol = 'BTCUSDT'
            coin_label = 'BTC'
        elif coin == 'ETH':
            symbol = 'ETHUSDT'
            coin_label = 'ETH'
        else:
            raise ValueError(f"ì§€ì›í•˜ì§€ ì•ŠëŠ” ì½”ì¸: {coin}")
        
        # Supabase ìš°ì„  ì‚¬ìš© (í´ë¼ìš°ë“œ í™˜ê²½)
        if self.use_supabase:
            try:
                supabase = self._get_supabase_client()
                if supabase:
                    # 1. binance_spot_weekly ë¡œë“œ
                    weekly_response = supabase.table("binance_spot_weekly") \
                        .select("*") \
                        .eq("symbol", symbol) \
                        .gte("date", start_date) \
                        .lte("date", end_date) \
                        .order("date") \
                        .execute()
                    
                    if not weekly_response.data or len(weekly_response.data) == 0:
                        return pd.DataFrame()
                    
                    df = pd.DataFrame(weekly_response.data)
                    df['date'] = pd.to_datetime(df['date'])
                    
                    # 2. bitinfocharts_whale_weekly ë³‘í•©
                    whale_weekly_response = supabase.table("bitinfocharts_whale_weekly") \
                        .select("*") \
                        .eq("coin", coin_label) \
                        .gte("week_end_date", start_date) \
                        .lte("week_end_date", end_date) \
                        .order("week_end_date") \
                        .execute()
                    
                    if whale_weekly_response.data and len(whale_weekly_response.data) > 0:
                        whale_df = pd.DataFrame(whale_weekly_response.data)
                        whale_df['date'] = pd.to_datetime(whale_df['week_end_date'])
                        df = pd.merge(df, whale_df[['date', 'avg_top100_richest_pct', 'avg_transaction_value_btc', 'whale_conc_change_7d']], 
                                    on='date', how='left')
                        df = df.rename(columns={
                            'avg_top100_richest_pct': 'top100_richest_pct',
                            'avg_transaction_value_btc': 'avg_transaction_value_btc'
                        })
                    
                    # 3. binance_futures_weekly ë³‘í•© (ìˆëŠ” ê²½ìš°)
                    # ì£¼ì˜: ì´ í…Œì´ë¸”ì´ Supabaseì— ì—†ì„ ìˆ˜ ìˆìŒ
                    try:
                        futures_weekly_response = supabase.table("binance_futures_weekly") \
                            .select("*") \
                            .eq("symbol", symbol) \
                            .gte("week_end_date", start_date) \
                            .lte("week_end_date", end_date) \
                            .order("week_end_date") \
                            .execute()
                        
                        if futures_weekly_response.data and len(futures_weekly_response.data) > 0:
                            futures_df = pd.DataFrame(futures_weekly_response.data)
                            futures_df['date'] = pd.to_datetime(futures_df['week_end_date'])
                            df = pd.merge(df, futures_df[['date', 'avg_funding_rate', 'sum_open_interest', 'oi_growth_7d', 'funding_rate_zscore']], 
                                        on='date', how='left')
                    except Exception as e:
                        logging.warning(f"binance_futures_weekly í…Œì´ë¸”ì´ ì—†ê±°ë‚˜ ì ‘ê·¼ ë¶ˆê°€: {e}")
                    
                    # ìˆ«ì ì»¬ëŸ¼ ë³€í™˜
                    numeric_columns = [
                        'open', 'high', 'low', 'close', 'volume', 'quote_volume',
                        'atr', 'rsi', 'volatility_ratio', 'weekly_range_pct',
                        'top100_richest_pct', 'avg_transaction_value_btc', 'whale_conc_change_7d',
                        'avg_funding_rate', 'sum_open_interest', 'oi_growth_7d', 'funding_rate_zscore'
                    ]
                    for col in numeric_columns:
                        if col in df.columns:
                            df[col] = pd.to_numeric(df[col], errors='coerce')
                    
                    # ì£¼ë´‰ íŠ¹ì„± ì¶”ê°€ ê³„ì‚°
                    if 'close' in df.columns:
                        df['weekly_return'] = df['close'].pct_change()
                    if 'high' in df.columns and 'low' in df.columns:
                        df['high_low_range'] = (df['high'] - df['low']) / df['low']
                    
                    # ì‹¤ì œ ê³ ë³€ë™ì„± íƒ€ê²Ÿ ë³€ìˆ˜ ê³„ì‚°
                    if 'volatility_ratio' in df.columns:
                        df['next_week_volatility'] = df['volatility_ratio'].shift(-1)
                        if df['volatility_ratio'].max() > 0:
                            quantile_threshold = df['volatility_ratio'].quantile(0.7)
                            absolute_threshold = df['volatility_ratio'].median() * 1.5
                            df['target_high_vol'] = (
                                (df['next_week_volatility'] > quantile_threshold) | 
                                (df['next_week_volatility'] > absolute_threshold)
                            ).astype(int)
                        else:
                            df['target_high_vol'] = 0
                        df['target_high_vol'] = df['target_high_vol'].fillna(0).astype(int)
                    
                    # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
                    df = df.ffill().bfill()
                    
                    return df
            except Exception as e:
                logging.warning(f"Supabaseì—ì„œ ì£¼ë´‰ ë°ì´í„° ë¡œë“œ ì‹¤íŒ¨, SQLiteë¡œ í´ë°±: {e}")
                # SQLiteë¡œ í´ë°±
        
        # SQLite ì‚¬ìš© (ë¡œì»¬ í™˜ê²½ ë˜ëŠ” Supabase ì‹¤íŒ¨ ì‹œ)
        try:
            if not hasattr(self, 'conn') or self.conn is None:
                logging.error("ë°ì´í„°ë² ì´ìŠ¤ ì—°ê²°ì´ ì—†ìŠµë‹ˆë‹¤")
                return pd.DataFrame()
            
            # ì£¼ë´‰ OHLCV + ì£¼ê°„ ê³ ë˜ ë°ì´í„° + ì£¼ê°„ ì„ ë¬¼ ë°ì´í„° JOIN
            query = f"""
            SELECT 
                w.date,
                w.symbol,
                w.open,
                w.high,
                w.low,
                w.close,
                w.volume,
                w.quote_volume,
                w.atr,
                w.rsi,
                w.volatility_ratio,
                w.weekly_range_pct,
                wh.avg_top100_richest_pct as top100_richest_pct,
                wh.avg_transaction_value_btc as avg_transaction_value_btc,
                wh.whale_conc_change_7d,
                fw.avg_funding_rate,
                fw.sum_open_interest,
                fw.oi_growth_7d,
                fw.funding_rate_zscore
            FROM binance_spot_weekly w
            LEFT JOIN bitinfocharts_whale_weekly wh 
                ON w.date = wh.week_end_date AND wh.coin = '{coin_label}'
            LEFT JOIN binance_futures_weekly fw
                ON w.date = fw.week_end_date AND fw.symbol = '{symbol}'
            WHERE w.symbol = '{symbol}'
            AND w.date BETWEEN '{start_date}' AND '{end_date}'
            ORDER BY w.date
            """
            
            df = pd.read_sql(query, self.conn)
            
            if len(df) == 0:
                return df
            
            df['date'] = pd.to_datetime(df['date'])
            
            # ìˆ«ì ì»¬ëŸ¼ì„ ëª…ì‹œì ìœ¼ë¡œ floatë¡œ ë³€í™˜
            numeric_columns = [
                'open', 'high', 'low', 'close', 'volume', 'quote_volume',
                'atr', 'rsi', 'volatility_ratio', 'weekly_range_pct',
                'top100_richest_pct', 'avg_transaction_value_btc', 'whale_conc_change_7d',
                'avg_funding_rate', 'sum_open_interest', 'oi_growth_7d', 'funding_rate_zscore'
            ]
            
            for col in numeric_columns:
                if col in df.columns:
                    df[col] = pd.to_numeric(df[col], errors='coerce')
            
            # ì£¼ë´‰ íŠ¹ì„± ì¶”ê°€ ê³„ì‚°
            df['weekly_return'] = df['close'].pct_change()
            df['high_low_range'] = (df['high'] - df['low']) / df['low']
            
            # ì‹¤ì œ ê³ ë³€ë™ì„± íƒ€ê²Ÿ ë³€ìˆ˜ ê³„ì‚° (ì£¼ë´‰ì— ë§ê²Œ ê°œì„ )
            # ë‹¤ìŒ ì£¼ì˜ ë³€ë™ì„±ì„ ê¸°ì¤€ìœ¼ë¡œ ê³ ë³€ë™ì„± ì—¬ë¶€ íŒë‹¨
            df['next_week_volatility'] = df['volatility_ratio'].shift(-1)
            
            # ì£¼ë´‰ì€ ì¼ë´‰ë³´ë‹¤ ë³€ë™ì„±ì´ í‰í™œí™”ë˜ë¯€ë¡œ ì„ê³„ê°’ì„ ë‚®ì¶¤
            if df['volatility_ratio'].max() > 0:
                # ìƒìœ„ 30%ë¡œ ì¡°ì • (ì¼ë´‰ì€ 20%, ì£¼ë´‰ì€ ë” ë„“ê²Œ)
                quantile_threshold = df['volatility_ratio'].quantile(0.7)
                # ì ˆëŒ€ ì„ê³„ê°’: ì¤‘ì•™ê°’ì˜ 1.5ë°° (ì£¼ë´‰ íŠ¹ì„± ë°˜ì˜)
                absolute_threshold = df['volatility_ratio'].median() * 1.5
                
                # ë‘ ì¡°ê±´ ì¤‘ í•˜ë‚˜ë¼ë„ ë§Œì¡±í•˜ë©´ ê³ ë³€ë™ì„±
                df['target_high_vol'] = (
                    (df['next_week_volatility'] > quantile_threshold) | 
                    (df['next_week_volatility'] > absolute_threshold)
                ).astype(int)
            else:
                df['target_high_vol'] = 0
            
            df['target_high_vol'] = df['target_high_vol'].fillna(0).astype(int)
            
            # ê²°ì¸¡ì¹˜ ì²˜ë¦¬
            df = df.ffill().bfill()
            
            return df
            
        except sqlite3.Error as e:
            error_msg = f"SQL ì˜¤ë¥˜ (load_risk_data_weekly): {str(e)}"
            logging.error(error_msg)
            return pd.DataFrame()
        except Exception as e:
            error_msg = f"load_risk_data_weekly ì˜¤ë¥˜: {str(e)}"
            logging.error(error_msg)
            return pd.DataFrame()
